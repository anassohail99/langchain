{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d002da79-d690-4ce3-9d3a-713256dc0ac6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install dotenv langchain langchain_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93e5b1c-a435-465c-8159-a608fa3c48f6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install -U langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c6f96ae-8b1f-4748-8b73-97eece7008c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85d87c00-69fd-4e78-8de1-ed7cce5e23f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGoogleGenerativeAI(model='models/gemini-2.0-flash', google_api_key=SecretStr('**********'), temperature=0.0, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x10b3b71a0>, default_metadata=())"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
    "model = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\", temperature=0)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ccf6e10-a7b9-4a9f-87d6-a07e14b924a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There isn't a single, universally accepted translation of \"Hi!\" in Hindko, as it depends on the context and the relationship between the speakers. Here are a few options:\n",
      "\n",
      "*   **Assalam-o-Alaikum (السلام علیکم):** This is the most common and respectful greeting, used in most situations. It means \"Peace be upon you.\"\n",
      "\n",
      "*   **Adaab (آداب):** This is another respectful greeting, similar to \"Hello\" or \"Greetings.\"\n",
      "\n",
      "*   **Kia haal hai? (کیا حال ہے؟):** This translates to \"How are you?\" and can be used as a greeting.\n",
      "\n",
      "*   **Theek ho? (ٹھیک ہو؟):** This translates to \"Are you well?\" and can also be used as a greeting, especially among friends.\n",
      "\n",
      "*   **Koi nawaan? (کوئی نواں؟):** This translates to \"What's new?\" and can be used as a casual greeting.\n",
      "\n",
      "So, the best translation depends on the situation. If you want to be polite and respectful, use **Assalam-o-Alaikum** or **Adaab**. If you're talking to a friend, you could use **Kia haal hai?**, **Theek ho?**, or **Koi nawaan?**.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(\"Translate the following from English into Hindko\"),\n",
    "    HumanMessage(\"Hi!\"),\n",
    "]\n",
    "\n",
    "print(model.invoke(messages).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe65419-bd63-4951-8c94-7680698c6422",
   "metadata": {},
   "source": [
    "# Streaming\n",
    "Streaming in LangChain refers to the ability to process and return data incrementally as it becomes available, rather than waiting for the entire response to be generated before returning it.\n",
    "\n",
    "1. Large Language Model (LLM) Responses:* When generating long text outputs, streaming allows you to show tokens as they're generated\n",
    "\n",
    "2. Real-time Processing: For applications that need to display or process information as it arrives\n",
    "\n",
    "3. Improved User Experience: Users see progress instead of waiting for complete responses\n",
    "\n",
    "## How Streaming Works in LangChain\n",
    "\n",
    "LangChain provides streaming capabilities through:\n",
    "\n",
    "1. Streaming Callbacks: Implementing callback handlers that process chunks of data as they arrive\n",
    "\n",
    "2. Streaming Interfaces: Special interfaces for models that support streaming outputs\n",
    "\n",
    "## Key Benefits\n",
    "Reduced Latency: Users see the first parts of the response immediately\n",
    "\n",
    "Memory Efficiency: Doesn't require storing the entire response in memory at once\n",
    "\n",
    "Interactive Experience: Enables more conversational interfaces\n",
    "\n",
    "Streaming is especially valuable for chat applications, real-time data processing, and any scenario where immediate feedback is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "daf20cd9-80b6-468e-9276-946863d353cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In| the context of AI systems, **streaming** refers to the continuous processing of data as it arrives|, rather than waiting for a complete dataset to be collected before processing begins. Think| of it like a river flowing continuously, instead of a lake that needs to fill up before you can use the water.\n",
      "\n",
      "Here's a breakdown of the concept:|\n",
      "\n",
      "**Key Characteristics of Streaming in AI:**\n",
      "\n",
      " processing where data is collected and processed in discrete chunks. stream.  This is in contrast to batch|\n",
      "*   **Real-time or Near Real-time Processing:**  The goal is to process data as quickly as possible after it arrives, enabling timely insights and actions.\n",
      " Latency:**  Minimizing the delay between data arrival and processing completion is crucial.\n",
      "*   **Scalability:**  The system must be able to handle varying data volumes and processing demands without significant performance degradation.\n",
      " Management:**  Streaming systems often need to maintain some form of state (e.g., running averages, counts, model parameters) to perform calculations across multiple data points in the stream.\n",
      "*   **Fault Tolerance:**  Streaming systems should be resilient to failures and able to recover from errors without losing data or interrupting processing.\n",
      "\n",
      "Why is Streaming Important in AI?**\n",
      "\n",
      "*   **Real-time Decision Making:**  Many AI applications require immediate responses based on the latest data.  For example:\n",
      "    *   **Fraud Detection:**  Identifying fraudulent transactions as they occur.\n",
      " road conditions in real-time.*  Reacting to changing|\n",
      "    *   **Personalized Recommendations:**  Suggesting products or content based on a user's current activity.\n",
      " entire dataset before processing.essing data in a streaming manner can be more efficient than batch processing, especially for large datasets.  You don't need to store the|\n",
      "*   **Adaptability:**  Streaming AI systems can adapt to changing data patterns and trends more quickly than batch-oriented systems.  Models can be continuously updated with new data.\n",
      " at a very high rate, such as sensor networks, social media feeds, and financial markets.s that generate data|\n",
      "\n",
      "**Examples of Streaming AI Applications:**\n",
      "\n",
      "*   **Real-time Anomaly Detection:** Identifying unusual patterns in network traffic, sensor data, or financial transactions.\n",
      " social media feeds or news articles in real-time to identify sentiment, trends, or emerging topics.\n",
      "*   **Computer Vision:** Processing video streams from security cameras or autonomous vehicles to detect objects, track movement, or identify events.\n",
      " series data, such as stock prices or weather patterns. based on historical time|\n",
      "*   **Personalized Recommendations:**  Providing real-time recommendations for products, content, or services based on a user's current activity and preferences.\n",
      "\n",
      "**Technologies Used in Streaming AI:**\n",
      "\n",
      " Streaming, Amazon Kinesis, Google Cloud Dataflow Flink, Apache Spark|\n",
      "*   **Message Queues:**  RabbitMQ, Redis\n",
      "*   **Databases:**  Time-series databases (e.g., InfluxDB, TimescaleDB), NoSQL databases (e.g., Cassandra, MongoDB)\n",
      " Learning Frameworks:** TensorFlow, PyTorch (often used with streaming platforms)\n",
      "\n",
      "**Challenges of Streaming AI:**\n",
      "\n",
      "*   **Complexity:**  Designing and implementing streaming AI systems can be more complex than batch processing systems.\n",
      "*   **State Management:**  Maintaining state across multiple data points in a stream can be challenging.|\n",
      "*   **Fault Tolerance:**  Ensuring that the system is resilient to failures and can recover from errors without losing data.\n",
      "*   **Data Quality:**  Dealing with noisy or incomplete data in a continuous stream.\n",
      " stability. Updates:**  Continuously updating machine learning models with new data while maintaining performance and|\n",
      "\n",
      "**In summary, streaming in AI systems is about processing data continuously as it arrives, enabling real-time insights, actions, and adaptability. It's a crucial approach for applications that require immediate responses and can handle high-velocity data streams.**|"
     ]
    }
   ],
   "source": [
    "for token in model.stream(\"Explain the concept of streaming in AI systems\"):\n",
    "    print(token.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7418f5-33ea-4f40-81e0-8ea4fb18931d",
   "metadata": {},
   "source": [
    "# Prompt Template from Messages in LangChain\n",
    "\n",
    "## Overview\n",
    "The `PromptTemplate.from_messages()` method creates structured conversation prompts by combining multiple message components with different roles. This is essential for chat models requiring conversation history or role-specific prompts.\n",
    "\n",
    "Best Practices\n",
    "Always include system message for initial instructions\n",
    "\n",
    "Use variables to make templates reusable\n",
    "\n",
    "Maintain proper message ordering (system → history → human)\n",
    "\n",
    "For long conversations, consider using MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9b8e307f-e43d-4ace-b757-9c62796c08fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt template using format message, You are a math tutor.\n",
      "Prompt template using format message, Explain quadratic equations in simple terms\n",
      "أهلاً! (Ahlan!)\n",
      "\n",
      "This is a general and friendly greeting. You could also say:\n",
      "\n",
      "*   مرحباً (Marhaban) - Also a common greeting.\n",
      "*   السلام عليكم (As-salamu alaykum) - A more formal and traditional greeting, meaning \"Peace be upon you.\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a {subject} tutor.\"),\n",
    "    (\"human\", \"Explain {topic} in simple terms\")\n",
    "])\n",
    "\n",
    "formatted = chat_prompt.format_messages(\n",
    "    subject=\"math\",\n",
    "    topic=\"quadratic equations\"\n",
    "    \n",
    ")\n",
    "print(\"Prompt template using format message,\",formatted[0].content)\n",
    "print(\"Prompt template using format message,\",formatted[1].content)\n",
    "\n",
    "\n",
    "# OR\n",
    "\n",
    "system_template = \"Translate the following from English into {language}\"\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")\n",
    "\n",
    "prompt = prompt_template.invoke({\"language\": \"arabic\", \"text\": \"hi!\"})\n",
    "response = model.invoke(prompt)\n",
    "print(response.content)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41845e5-7607-4dc5-b5ca-1f26f9d479c8",
   "metadata": {},
   "source": [
    "# PromptTemplate.from_strings() in LangChain\n",
    "\n",
    "## Overview\n",
    "The `PromptTemplate.from_strings()` method creates a prompt template from a sequence of string templates. This is useful for constructing prompts from multiple text components that need to be combined.\n",
    "\n",
    "## Key Features\n",
    "- Combines multiple string fragments into a single prompt\n",
    "\n",
    "- Supports template variables in individual strings\n",
    "\n",
    "- Preserves the exact ordering of input strings\n",
    "\n",
    "- Automatically joins strings with spaces by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0ed634dd-c026-4aae-a225-9f5ad52b34a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the cat join the Red Cross?\n",
      "\n",
      "Because he wanted to be a first-aid kit!\n"
     ]
    }
   ],
   "source": [
    "prompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "prompt = prompt_template.invoke({\"topic\": \"cats\"})\n",
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "af56e719-3b80-4c65-865b-3ec512558dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One impressive fact about Roman Reigns in WWE is that he holds the record for the longest Universal Championship reign at 1,316 days.\n"
     ]
    }
   ],
   "source": [
    "prompt_template = PromptTemplate.from_template(\"Give me one good facts about {wrestler_name} in {wrestling_company}\")\n",
    "prompt =  prompt_template.invoke({\"wrestler_name\":\"roman reigns\" ,\"wrestling_company\":\"wwe\"})\n",
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b021e5eb-f791-4289-a903-57df8a8535da",
   "metadata": {},
   "source": [
    "# `from_template()` vs `from_messages()` in LangChain\n",
    "\n",
    "## Key Differences\n",
    "\n",
    "| Feature                | `from_template()`                            | `from_messages()`                            |\n",
    "|------------------------|---------------------------------------------|---------------------------------------------|\n",
    "| **Primary Use Case**   | Simple single-text prompts                  | Structured conversation prompts             |\n",
    "| **Input Format**       | Single string template                      | List of (role, content) tuples              |\n",
    "| **Role Support**       | No role separation                          | Explicit roles (system/human/ai/custom)     |\n",
    "| **Variable Handling**  | Variables in one template                   | Variables can be role-specific              |\n",
    "| **Output Structure**   | Flat text prompt                            | Structured chat history format              |\n",
    "| **Best For**           | One-off queries, simple LLM interactions    | Chat applications, multi-turn conversations |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23fd1d8-ed76-4fe0-910d-d4aa7b707e66",
   "metadata": {},
   "source": [
    "# Runnable Interfaces in LangChain\n",
    "\n",
    "## Overview\n",
    "Runnables are the fundamental building blocks in LangChain that encapsulate executable units of work. They provide a standardized interface for operations that process input data to produce output.\n",
    "\n",
    "## Core Concepts\n",
    "\n",
    "### Key Characteristics\n",
    "- **Uniform Interface**: All Runnables implement `.invoke()`, `.stream()`, and `.batch()`\n",
    "- **Composable**: Can be chained together using the pipe (`|`) operator\n",
    "- **Async Support**: Native asynchronous execution\n",
    "- **Batch Processing**: Efficient handling of multiple inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0836943b-d71b-4061-af5d-7dda4aecd1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the scarecrow win an award?\n",
      "\n",
      "Because he was outstanding in his field! My Uncle told me that one. He's always corny.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "chain = prompt | model | StrOutputParser()\n",
    "print(chain.invoke({\"topic\": \"uncles\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e0483f-09cb-40d5-b07b-535aee7794db",
   "metadata": {},
   "source": [
    "## Coercion\n",
    "We can even combine this chain with more runnables to create another chain. This may involve some input/output formatting using other types of runnables, depending on the required inputs and outputs of the chain components.\n",
    "\n",
    "For example, let's say we wanted to compose the joke generating chain with another chain that evaluates whether or not the generated joke was funny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fb8fa156-fa2d-4b0f-978f-adf99469e113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, that's a classic pun and a funny joke! Here's why:\n",
      "\n",
      "*   **Pun:** The humor comes from the double meaning of \"outstanding in his field.\" It literally means he's physically standing out in a field, but it also means he's exceptionally good at what he does.\n",
      "*   **Corny:** It's definitely a corny joke, which adds to the humor. The fact that your uncle is \"always corny\" makes it even funnier because it fits his personality.\n",
      "*   **Relatability:** Most people have heard similar puns, so it's easily understood and appreciated.\n",
      "\n",
      "So, yes, it's a funny joke, especially because of its corny nature and the context of your uncle telling it.\n"
     ]
    }
   ],
   "source": [
    "analysis_prompt = ChatPromptTemplate.from_template(\"is this a funny joke? {joke}\")\n",
    "composed_chain = {\"joke\": chain} | analysis_prompt | model | StrOutputParser()\n",
    "print(composed_chain.invoke({\"topic\": \"uncles\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7998e0d-c7bf-4592-b00e-b632411f4288",
   "metadata": {},
   "source": [
    "# RunnableParallel in LangChain\n",
    "\n",
    "## Overview\n",
    "`RunnableParallel` (formerly known as `RunnableMap`) enables parallel execution of multiple Runnables. It takes a single input and distributes it to several Runnables simultaneously, returning a dictionary of their outputs.\n",
    "\n",
    "## Key Features\n",
    "- **Parallel Execution**: Runs multiple Runnables concurrently\n",
    "- **Dictionary Output**: Returns results in a structured format\n",
    "- **Input Broadcasting**: Same input sent to all branches\n",
    "- **Type Safety**: Maintains input/output type signatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ddfc99-5292-441d-9459-c2ac12d5efe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8a9766da-2129-4e78-8a43-c82dc7bad37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'double': 30, 'triple': 45}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def add_five(x): return x + 5\n",
    "sequence = RunnableLambda(add_five) | {\"double\": lambda x: x*2, \"triple\": lambda x: x*3}\n",
    "print(sequence.invoke(10))  # What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "67aac588-20af-43b2-8846-74fd532eaa2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 6, 8]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def add_one(x: int) -> int:\n",
    "    return x + 1\n",
    "\n",
    "def mul_two(x: int) -> int:\n",
    "    return x * 2\n",
    "\n",
    "runnable_1 = RunnableLambda(add_one)\n",
    "runnable_2 = RunnableLambda(mul_two)\n",
    "sequence = runnable_1 | runnable_2\n",
    "# Or equivalently:\n",
    "# sequence = RunnableSequence(first=runnable_1, last=runnable_2)\n",
    "sequence.invoke(1)\n",
    "await sequence.ainvoke(1)\n",
    "\n",
    "sequence.batch([1, 2, 3])\n",
    "await sequence.abatch([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8167b133-c4af-4adc-a2c8-0207c163e6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'mul_two': 4, 'mul_three': 6},\n",
       " {'mul_two': 6, 'mul_three': 9},\n",
       " {'mul_two': 8, 'mul_three': 12}]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def add_one(x: int) -> int:\n",
    "    return x + 1\n",
    "\n",
    "def mul_two(x: int) -> int:\n",
    "    return x * 2\n",
    "\n",
    "def mul_three(x: int) -> int:\n",
    "    return x * 3\n",
    "\n",
    "runnable_1 = RunnableLambda(add_one)\n",
    "runnable_2 = RunnableLambda(mul_two)\n",
    "runnable_3 = RunnableLambda(mul_three)\n",
    "\n",
    "sequence = runnable_1 | {  # this dict is coerced to a RunnableParallel\n",
    "    \"mul_two\": runnable_2,\n",
    "    \"mul_three\": runnable_3,\n",
    "}\n",
    "\n",
    "# Or equivalently:\n",
    "# sequence = runnable_1 | RunnableParallel(\n",
    "#     {\"mul_two\": runnable_2, \"mul_three\": runnable_3}\n",
    "# )\n",
    "# Also equivalently:\n",
    "# sequence = runnable_1 | RunnableParallel(\n",
    "#     mul_two=runnable_2,\n",
    "#     mul_three=runnable_3,\n",
    "# )\n",
    "\n",
    "sequence.invoke(1)\n",
    "await sequence.ainvoke(1)\n",
    "\n",
    "sequence.batch([1, 2, 3])\n",
    "await sequence.abatch([1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c8bfca-3b88-43a0-85d3-c996344173d9",
   "metadata": {},
   "source": [
    "# RunnableBranch in LangChain\n",
    "\n",
    "## Overview\n",
    "`RunnableBranch` provides conditional routing within LangChain workflows, allowing different Runnables to be executed based on input conditions.\n",
    "\n",
    "## Basic Syntax\n",
    "```python\n",
    "from langchain.schema.runnable import RunnableBranch\n",
    "\n",
    "branch = RunnableBranch(\n",
    "    (condition1, runnable1),\n",
    "    (condition2, runnable2),\n",
    "    default_runnable\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "876b7567-1edd-4d76-bcfb-ff57461c203b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableBranch\n",
    "\n",
    "branch = RunnableBranch(\n",
    "    (lambda x: isinstance(x, str), lambda x: x.upper()),\n",
    "    (lambda x: isinstance(x, int), lambda x: x + 1),\n",
    "    (lambda x: isinstance(x, float), lambda x: x * 2),\n",
    "    lambda x: \"goodbye\",\n",
    ")\n",
    "\n",
    "branch.invoke(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d9745ab7-fbb3-47e2-8f06-65bae16b1ef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HELLO'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "branch.invoke(\"hello\") # \"HELLO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "14a915d0-2e9a-4e4a-82fa-e4986c9f05ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'goodbye'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "branch.invoke(None) # \"goodbye\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139c5154-0925-4c81-8784-8e36dea07f94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
