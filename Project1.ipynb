{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7TKnUjalDm3AGrnCZ9/4U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anassohail99/langchain/blob/main/Project1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5Ii0bDX_N31Q",
        "outputId": "13e530e5-5765-42c9-ee0d-11b406dad927"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.13)\n",
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.10/dist-packages (2.0.7)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.26 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.28)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.3)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.3)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.10.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain-google-genai) (1.2.0)\n",
            "Requirement already satisfied: google-generativeai<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from langchain-google-genai) (0.8.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.10 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.6.10)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.155.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.25.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.10->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.25.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.26->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.26->langchain) (24.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.66.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.9)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.26->langchain) (3.0.0)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.1.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.68.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (3.2.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-google-genai\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationChain\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "b8y6cOkOOAZl"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "W6NfxSP7OLwx"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "llm: ChatGoogleGenerativeAI = ChatGoogleGenerativeAI(\n",
        "    model='gemini-1.5-flash',\n",
        "    api_key=GOOGLE_API_KEY,\n",
        "    temperature=0.5\n",
        ")\n",
        "memory"
      ],
      "metadata": {
        "id": "rpDYNrssOmWq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5336d6b6-d925-4017-9264-051ba775b991"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConversationBufferMemory(chat_memory=InMemoryChatMessageHistory(messages=[]), return_messages=True)"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"You are a helpful assistant. Answer the following question:\\n\\n{question}\"\n",
        ")\n",
        "prompt_template.template"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "EYlq3SR6QEIb",
        "outputId": "bd61e8e4-28ff-4883-a414-07cc0cc4a179"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You are a helpful assistant. Answer the following question:\\n\\n{question}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fG8kB_C7f4Mj"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt_template | llm\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "ZbZ71CyWPRAY"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = chain.invoke(\"What is langchain and how it is helpful?\")\n",
        "result.content"
      ],
      "metadata": {
        "id": "_a4qn3WtP_P7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "ecbd5a0f-dd3a-4951-cad8-20a26a823307"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'LangChain is an open-source framework designed to simplify the development of applications powered by large language models (LLMs).  Instead of treating LLMs as isolated components, LangChain provides a structured way to connect them with other sources of data and computation, creating more powerful and useful applications.\\n\\nHere\\'s how it\\'s helpful:\\n\\n* **Modular Design:** LangChain breaks down complex LLM applications into manageable modules. This makes it easier to build, test, and maintain complex systems.  It allows developers to focus on specific parts of the application without getting bogged down in the intricacies of integrating different components.\\n\\n* **Chain Building:**  It enables the creation of \"chains,\" which are sequences of operations involving LLMs and other components.  This allows for more sophisticated workflows, such as:\\n    * **Prompt chaining:**  Breaking down a complex task into smaller prompts fed sequentially to the LLM.\\n    * **LLM + tools chaining:** Combining LLMs with external tools (e.g., search engines, databases, calculators) to access and process information beyond the LLM\\'s knowledge base.\\n    * **Memory:**  Maintaining context across multiple interactions with the LLM, enabling more coherent and relevant conversations.\\n\\n* **Agent Development:** LangChain facilitates the creation of agents that can autonomously choose which tools to use to complete a given task. This allows for more flexible and adaptable applications.\\n\\n* **Data Connection:**  LangChain simplifies connecting LLMs to various data sources, such as documents, databases, and APIs.  This allows LLMs to access and process information relevant to specific tasks, improving accuracy and relevance.\\n\\n* **Abstraction and Ease of Use:** LangChain provides a higher-level abstraction over LLMs, making it easier to work with different models and providers without needing to understand the specific APIs of each one.  This reduces development time and effort.\\n\\n\\nIn short, LangChain helps developers build more complex and powerful applications using LLMs by providing a structured, modular, and easy-to-use framework for connecting LLMs with other components and data sources.  It accelerates development, improves code maintainability, and unlocks the full potential of LLMs for a wider range of applications.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = conversation.invoke(input=\"What is LangChain?\")\n",
        "response"
      ],
      "metadata": {
        "id": "psOvWP9gdhGF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22f1df38-5a95-4a81-9d17-69d726834028"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[HumanMessage(content='What is LangChain?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"LangChain is a framework for developing applications powered by large language models (LLMs).  Think of it as a toolbox filled with different components that you can assemble to create sophisticated and useful applications.  These components handle things like:\\n\\n* **Connecting to LLMs:** LangChain makes it easy to interface with various LLMs, including OpenAI's models (like GPT-3, GPT-4), Hugging Face models, and others.  It abstracts away the complexities of making API calls and managing different API keys.\\n\\n* **Managing Memory:**  LLMs are stateless, meaning they don't remember previous interactions within a single conversation. LangChain provides different memory mechanisms to allow your applications to maintain context across multiple turns, improving the coherence and usefulness of the LLM's responses.  These memory types include short-term memory (like remembering the last few messages) and long-term memory (like storing information from previous sessions).\\n\\n* **Chains:**  This is a core concept in LangChain. Chains are sequences of calls to different components, often involving LLMs and other tools.  For example, a chain might involve:  (1) asking an LLM to generate a question, (2) using a search engine to find the answer, and (3) asking the LLM to summarize the answer.  This allows you to build complex workflows.\\n\\n* **Agents:**  Agents are more sophisticated than chains. They decide which tools to use to answer a user's query.  They might choose to use a search engine, a calculator, or an LLM, depending on the context.  This makes your applications more adaptable and capable of handling a wider range of tasks.\\n\\n* **Indexes:**  LangChain helps you index and query your own data.  You can feed your documents (like PDFs, web pages, etc.) into LangChain, and it will create an index that allows the LLM to access and process that information.  This is crucial for building applications that leverage your own private data.\\n\\nIn short, LangChain simplifies the process of building LLM-powered applications by providing pre-built components and tools for managing the complexities of interacting with LLMs and other external resources. It's designed to be modular and extensible, allowing developers to customize and combine components to create unique applications.  It's not an LLM itself, but rather a framework for *using* LLMs effectively.\\n\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is LangChain?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"LangChain is a framework for developing applications powered by large language models (LLMs).  Think of it as a toolbox filled with different components that you can assemble to create sophisticated and useful applications.  These components handle things like:\\n\\n* **Connecting to LLMs:** LangChain makes it easy to interface with various LLMs, including OpenAI's models (like GPT-3, GPT-4), Hugging Face models, and others.  It abstracts away the complexities of making API calls and managing different API keys.  For example, you don't need to write custom code to handle authentication and rate limits for each LLM; LangChain provides a consistent interface.\\n\\n* **Managing Memory:**  LLMs are stateless, meaning they don't remember previous interactions within a single conversation. LangChain provides different memory mechanisms to allow your applications to maintain context across multiple turns, improving the coherence and usefulness of the LLM's responses.  These memory types include short-term memory (like remembering the last few messages using techniques like conversation buffers) and long-term memory (like storing information from previous sessions in a vector database).  This is particularly important for building conversational AI applications.\\n\\n* **Chains:**  This is a core concept in LangChain. Chains are sequences of calls to different components, often involving LLMs and other tools.  For example, a chain might involve:  (1) asking an LLM to generate a question, (2) using a search engine (like Google Search or SerpAPI) to find the answer, and (3) asking the LLM to summarize the answer.  This allows you to build complex workflows where the output of one component becomes the input of another.  LangChain provides pre-built chain types and allows you to create custom chains.\\n\\n* **Agents:**  Agents are more sophisticated than chains. They decide which tools to use to answer a user's query.  They might choose to use a search engine, a calculator, a database, or an LLM, depending on the context of the user's question.  The agent selects the appropriate tool based on a predefined set of rules or a learned policy. This makes your applications more adaptable and capable of handling a wider range of tasks.  For example, an agent might decide to use a calculator for a math problem and a search engine for a factual question.\\n\\n* **Indexes:**  LangChain helps you index and query your own data.  You can feed your documents (like PDFs, web pages, CSV files, etc.) into LangChain, and it will create an index (using techniques like embedding the text and storing it in a vector database) that allows the LLM to access and process that information.  This is crucial for building applications that leverage your own private data, enabling the LLM to answer questions based on your specific documents.  This avoids the need to manually parse and process your data.\\n\\nIn short, LangChain simplifies the process of building LLM-powered applications by providing pre-built components and tools for managing the complexities of interacting with LLMs and other external resources. It's designed to be modular and extensible, allowing developers to customize and combine components to create unique applications.  It's not an LLM itself, but rather a framework for *using* LLMs effectively.\\n\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is LangChain?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"LangChain is a framework for building applications powered by large language models (LLMs).  Think of it as a toolbox filled with various components you can assemble to create sophisticated applications.  These components handle key aspects of LLM application development:\\n\\n* **Connecting to LLMs:** LangChain simplifies connecting to different LLMs like OpenAI's GPT-3/4, Hugging Face models, etc.  It manages API calls, authentication, and rate limits, providing a consistent interface regardless of the specific LLM.  You don't need to write custom code for each LLM's API.\\n\\n* **Managing Memory:** Since LLMs are stateless, LangChain offers memory mechanisms to maintain context across multiple turns in a conversation.  This includes short-term memory (like remembering recent messages in a buffer) and long-term memory (persisting information across sessions, often using vector databases).  This is vital for building coherent conversational AI.\\n\\n* **Chains:**  These are sequences of calls to different components, often involving LLMs and other tools. A chain might involve: (1) an LLM generating a question, (2) a search engine finding the answer, and (3) the LLM summarizing the answer.  LangChain provides pre-built chain types and lets you create custom ones for complex workflows.\\n\\n* **Agents:**  Agents are more advanced than chains. They *decide* which tool (search engine, calculator, database, LLM, etc.) to use based on a user's query, adapting to different contexts.  This makes applications more versatile and capable of handling diverse tasks.\\n\\n* **Indexes:** LangChain helps you index your own data (PDFs, web pages, CSV files). It creates an index (often using embeddings and a vector database) allowing the LLM to access and process this information, enabling applications to leverage private data.\\n\\n\\nIn essence, LangChain simplifies building LLM applications by offering pre-built components and tools to manage the complexities of interacting with LLMs and external resources.  It's modular and extensible, letting developers customize and combine components.  It's not an LLM itself, but a framework for *using* LLMs effectively.\\n\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is LangChain?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"LangChain is a framework for building applications powered by large language models (LLMs).  Imagine it as a toolbox filled with pre-built components that you can assemble to create sophisticated applications.  These components handle several key aspects:\\n\\n* **Connecting to LLMs:** LangChain simplifies the process of connecting to various LLMs, such as OpenAI's GPT-3, GPT-4, and models from Hugging Face.  It abstracts away the complexities of API calls, authentication, and rate limits, providing a consistent interface regardless of the specific LLM you're using.  This means you don't have to write unique code for each LLM's API.\\n\\n* **Managing Memory:** Because LLMs are stateless (they don't inherently remember past interactions), LangChain provides memory mechanisms.  These allow your application to maintain context across multiple turns in a conversation.  This includes short-term memory (like keeping track of the last few messages) and long-term memory (persisting information across sessions, often using vector databases). This is crucial for building coherent conversational AI.\\n\\n* **Chains:**  Chains are sequences of calls to different components, often involving LLMs and other tools. A simple example:  (1) an LLM generates a question; (2) a search engine finds the answer; (3) the LLM summarizes the answer. LangChain offers pre-built chain types and allows you to create custom chains for complex workflows.\\n\\n* **Agents:** Agents are more advanced than chains.  They *decide* which tool to use (search engine, calculator, database, another LLM, etc.) based on the user's query.  This adaptability makes applications more versatile and capable of handling a wider range of tasks.\\n\\n* **Indexes:** LangChain helps you index your own data (like PDFs, web pages, or CSV files).  It creates an index (frequently using embeddings and a vector database) allowing the LLM to access and process this information. This is essential for applications that need to leverage private data.\\n\\nIn short, LangChain simplifies the development of LLM-powered applications by providing pre-built components and tools to manage the complexities of interacting with LLMs and external resources. It's designed to be modular and extensible, allowing developers to customize and combine components to build unique applications.  It's not an LLM itself; it's a framework for *using* LLMs effectively.\\n\", additional_kwargs={}, response_metadata={})]\n",
            "Human: What is LangChain?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is LangChain?',\n",
              " 'history': [HumanMessage(content='What is LangChain?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"LangChain is a framework for developing applications powered by large language models (LLMs).  Think of it as a toolbox filled with different components that you can assemble to create sophisticated and useful applications.  These components handle things like:\\n\\n* **Connecting to LLMs:** LangChain makes it easy to interface with various LLMs, including OpenAI's models (like GPT-3, GPT-4), Hugging Face models, and others.  It abstracts away the complexities of making API calls and managing different API keys.\\n\\n* **Managing Memory:**  LLMs are stateless, meaning they don't remember previous interactions within a single conversation. LangChain provides different memory mechanisms to allow your applications to maintain context across multiple turns, improving the coherence and usefulness of the LLM's responses.  These memory types include short-term memory (like remembering the last few messages) and long-term memory (like storing information from previous sessions).\\n\\n* **Chains:**  This is a core concept in LangChain. Chains are sequences of calls to different components, often involving LLMs and other tools.  For example, a chain might involve:  (1) asking an LLM to generate a question, (2) using a search engine to find the answer, and (3) asking the LLM to summarize the answer.  This allows you to build complex workflows.\\n\\n* **Agents:**  Agents are more sophisticated than chains. They decide which tools to use to answer a user's query.  They might choose to use a search engine, a calculator, or an LLM, depending on the context.  This makes your applications more adaptable and capable of handling a wider range of tasks.\\n\\n* **Indexes:**  LangChain helps you index and query your own data.  You can feed your documents (like PDFs, web pages, etc.) into LangChain, and it will create an index that allows the LLM to access and process that information.  This is crucial for building applications that leverage your own private data.\\n\\nIn short, LangChain simplifies the process of building LLM-powered applications by providing pre-built components and tools for managing the complexities of interacting with LLMs and other external resources. It's designed to be modular and extensible, allowing developers to customize and combine components to create unique applications.  It's not an LLM itself, but rather a framework for *using* LLMs effectively.\\n\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='What is LangChain?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"LangChain is a framework for developing applications powered by large language models (LLMs).  Think of it as a toolbox filled with different components that you can assemble to create sophisticated and useful applications.  These components handle things like:\\n\\n* **Connecting to LLMs:** LangChain makes it easy to interface with various LLMs, including OpenAI's models (like GPT-3, GPT-4), Hugging Face models, and others.  It abstracts away the complexities of making API calls and managing different API keys.  For example, you don't need to write custom code to handle authentication and rate limits for each LLM; LangChain provides a consistent interface.\\n\\n* **Managing Memory:**  LLMs are stateless, meaning they don't remember previous interactions within a single conversation. LangChain provides different memory mechanisms to allow your applications to maintain context across multiple turns, improving the coherence and usefulness of the LLM's responses.  These memory types include short-term memory (like remembering the last few messages using techniques like conversation buffers) and long-term memory (like storing information from previous sessions in a vector database).  This is particularly important for building conversational AI applications.\\n\\n* **Chains:**  This is a core concept in LangChain. Chains are sequences of calls to different components, often involving LLMs and other tools.  For example, a chain might involve:  (1) asking an LLM to generate a question, (2) using a search engine (like Google Search or SerpAPI) to find the answer, and (3) asking the LLM to summarize the answer.  This allows you to build complex workflows where the output of one component becomes the input of another.  LangChain provides pre-built chain types and allows you to create custom chains.\\n\\n* **Agents:**  Agents are more sophisticated than chains. They decide which tools to use to answer a user's query.  They might choose to use a search engine, a calculator, a database, or an LLM, depending on the context of the user's question.  The agent selects the appropriate tool based on a predefined set of rules or a learned policy. This makes your applications more adaptable and capable of handling a wider range of tasks.  For example, an agent might decide to use a calculator for a math problem and a search engine for a factual question.\\n\\n* **Indexes:**  LangChain helps you index and query your own data.  You can feed your documents (like PDFs, web pages, CSV files, etc.) into LangChain, and it will create an index (using techniques like embedding the text and storing it in a vector database) that allows the LLM to access and process that information.  This is crucial for building applications that leverage your own private data, enabling the LLM to answer questions based on your specific documents.  This avoids the need to manually parse and process your data.\\n\\nIn short, LangChain simplifies the process of building LLM-powered applications by providing pre-built components and tools for managing the complexities of interacting with LLMs and other external resources. It's designed to be modular and extensible, allowing developers to customize and combine components to create unique applications.  It's not an LLM itself, but rather a framework for *using* LLMs effectively.\\n\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='What is LangChain?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"LangChain is a framework for building applications powered by large language models (LLMs).  Think of it as a toolbox filled with various components you can assemble to create sophisticated applications.  These components handle key aspects of LLM application development:\\n\\n* **Connecting to LLMs:** LangChain simplifies connecting to different LLMs like OpenAI's GPT-3/4, Hugging Face models, etc.  It manages API calls, authentication, and rate limits, providing a consistent interface regardless of the specific LLM.  You don't need to write custom code for each LLM's API.\\n\\n* **Managing Memory:** Since LLMs are stateless, LangChain offers memory mechanisms to maintain context across multiple turns in a conversation.  This includes short-term memory (like remembering recent messages in a buffer) and long-term memory (persisting information across sessions, often using vector databases).  This is vital for building coherent conversational AI.\\n\\n* **Chains:**  These are sequences of calls to different components, often involving LLMs and other tools. A chain might involve: (1) an LLM generating a question, (2) a search engine finding the answer, and (3) the LLM summarizing the answer.  LangChain provides pre-built chain types and lets you create custom ones for complex workflows.\\n\\n* **Agents:**  Agents are more advanced than chains. They *decide* which tool (search engine, calculator, database, LLM, etc.) to use based on a user's query, adapting to different contexts.  This makes applications more versatile and capable of handling diverse tasks.\\n\\n* **Indexes:** LangChain helps you index your own data (PDFs, web pages, CSV files). It creates an index (often using embeddings and a vector database) allowing the LLM to access and process this information, enabling applications to leverage private data.\\n\\n\\nIn essence, LangChain simplifies building LLM applications by offering pre-built components and tools to manage the complexities of interacting with LLMs and external resources.  It's modular and extensible, letting developers customize and combine components.  It's not an LLM itself, but a framework for *using* LLMs effectively.\\n\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='What is LangChain?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"LangChain is a framework for building applications powered by large language models (LLMs).  Imagine it as a toolbox filled with pre-built components that you can assemble to create sophisticated applications.  These components handle several key aspects:\\n\\n* **Connecting to LLMs:** LangChain simplifies the process of connecting to various LLMs, such as OpenAI's GPT-3, GPT-4, and models from Hugging Face.  It abstracts away the complexities of API calls, authentication, and rate limits, providing a consistent interface regardless of the specific LLM you're using.  This means you don't have to write unique code for each LLM's API.\\n\\n* **Managing Memory:** Because LLMs are stateless (they don't inherently remember past interactions), LangChain provides memory mechanisms.  These allow your application to maintain context across multiple turns in a conversation.  This includes short-term memory (like keeping track of the last few messages) and long-term memory (persisting information across sessions, often using vector databases). This is crucial for building coherent conversational AI.\\n\\n* **Chains:**  Chains are sequences of calls to different components, often involving LLMs and other tools. A simple example:  (1) an LLM generates a question; (2) a search engine finds the answer; (3) the LLM summarizes the answer. LangChain offers pre-built chain types and allows you to create custom chains for complex workflows.\\n\\n* **Agents:** Agents are more advanced than chains.  They *decide* which tool to use (search engine, calculator, database, another LLM, etc.) based on the user's query.  This adaptability makes applications more versatile and capable of handling a wider range of tasks.\\n\\n* **Indexes:** LangChain helps you index your own data (like PDFs, web pages, or CSV files).  It creates an index (frequently using embeddings and a vector database) allowing the LLM to access and process this information. This is essential for applications that need to leverage private data.\\n\\nIn short, LangChain simplifies the development of LLM-powered applications by providing pre-built components and tools to manage the complexities of interacting with LLMs and external resources. It's designed to be modular and extensible, allowing developers to customize and combine components to build unique applications.  It's not an LLM itself; it's a framework for *using* LLMs effectively.\\n\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='What is LangChain?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"LangChain is a framework for building applications powered by large language models (LLMs).  Think of it as a toolbox filled with components you can assemble to create sophisticated applications.  These key components handle:\\n\\n* **Connecting to LLMs:** LangChain simplifies connecting to various LLMs like OpenAI's GPT-3/4, Hugging Face models, etc.  It manages API calls, authentication, and rate limits, offering a consistent interface regardless of the specific LLM. You don't write unique code for each LLM's API.\\n\\n* **Managing Memory:**  LLMs are stateless. LangChain provides memory mechanisms (short-term, like remembering recent messages; long-term, persisting information across sessions, often using vector databases) to maintain context across multiple conversational turns.  This is vital for coherent conversational AI.\\n\\n* **Chains:** These are sequences of calls to different components (often LLMs and other tools).  A chain might involve: (1) an LLM generating a question; (2) a search engine finding the answer; (3) the LLM summarizing the answer. LangChain offers pre-built and custom chain types for complex workflows.\\n\\n* **Agents:** Agents are advanced chains that *decide* which tool (search engine, calculator, database, another LLM) to use based on a user's query, adapting to different contexts for versatile application capabilities.\\n\\n* **Indexes:** LangChain helps index your own data (PDFs, web pages, CSV files) creating an index (often using embeddings and a vector database) that allows the LLM to access and process this information, enabling applications to leverage private data.\\n\\n\\nIn short, LangChain simplifies LLM application development by providing pre-built components and tools to manage the complexities of interacting with LLMs and external resources. It's modular, extensible, and allows developers to customize and combine components.  It's not an LLM itself, but a framework for *using* LLMs effectively.\\n\", additional_kwargs={}, response_metadata={})],\n",
              " 'response': \"LangChain is a framework for building applications powered by large language models (LLMs).  Think of it as a toolbox filled with components you can assemble to create sophisticated applications.  These key components handle:\\n\\n* **Connecting to LLMs:** LangChain simplifies connecting to various LLMs like OpenAI's GPT-3/4, Hugging Face models, etc.  It manages API calls, authentication, and rate limits, offering a consistent interface regardless of the specific LLM. You don't write unique code for each LLM's API.\\n\\n* **Managing Memory:**  LLMs are stateless. LangChain provides memory mechanisms (short-term, like remembering recent messages; long-term, persisting information across sessions, often using vector databases) to maintain context across multiple conversational turns.  This is vital for coherent conversational AI.\\n\\n* **Chains:** These are sequences of calls to different components (often LLMs and other tools).  A chain might involve: (1) an LLM generating a question; (2) a search engine finding the answer; (3) the LLM summarizing the answer. LangChain offers pre-built and custom chain types for complex workflows.\\n\\n* **Agents:** Agents are advanced chains that *decide* which tool (search engine, calculator, database, another LLM) to use based on a user's query, adapting to different contexts for versatile application capabilities.\\n\\n* **Indexes:** LangChain helps index your own data (PDFs, web pages, CSV files) creating an index (often using embeddings and a vector database) that allows the LLM to access and process this information, enabling applications to leverage private data.\\n\\n\\nIn short, LangChain simplifies LLM application development by providing pre-built components and tools to manage the complexities of interacting with LLMs and external resources. It's modular, extensible, and allows developers to customize and combine components.  It's not an LLM itself, but a framework for *using* LLMs effectively.\\n\"}"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(conversation.invoke(input=\"How does it work?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuxFC4VZgcIx",
        "outputId": "9542fb8f-33e8-4d7f-df07-823f6ae9ef88"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[HumanMessage(content='What is LangChain?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"LangChain is a framework for developing applications powered by large language models (LLMs).  Think of it as a toolbox filled with different components that you can assemble to create sophisticated and useful applications.  These components handle things like:\\n\\n* **Connecting to LLMs:** LangChain makes it easy to interface with various LLMs, including OpenAI's models (like GPT-3, GPT-4), Hugging Face models, and others.  It abstracts away the complexities of making API calls and managing different API keys.\\n\\n* **Managing Memory:**  LLMs are stateless, meaning they don't remember previous interactions within a single conversation. LangChain provides different memory mechanisms to allow your applications to maintain context across multiple turns, improving the coherence and usefulness of the LLM's responses.  These memory types include short-term memory (like remembering the last few messages) and long-term memory (like storing information from previous sessions).\\n\\n* **Chains:**  This is a core concept in LangChain. Chains are sequences of calls to different components, often involving LLMs and other tools.  For example, a chain might involve:  (1) asking an LLM to generate a question, (2) using a search engine to find the answer, and (3) asking the LLM to summarize the answer.  This allows you to build complex workflows.\\n\\n* **Agents:**  Agents are more sophisticated than chains. They decide which tools to use to answer a user's query.  They might choose to use a search engine, a calculator, or an LLM, depending on the context.  This makes your applications more adaptable and capable of handling a wider range of tasks.\\n\\n* **Indexes:**  LangChain helps you index and query your own data.  You can feed your documents (like PDFs, web pages, etc.) into LangChain, and it will create an index that allows the LLM to access and process that information.  This is crucial for building applications that leverage your own private data.\\n\\nIn short, LangChain simplifies the process of building LLM-powered applications by providing pre-built components and tools for managing the complexities of interacting with LLMs and other external resources. It's designed to be modular and extensible, allowing developers to customize and combine components to create unique applications.  It's not an LLM itself, but rather a framework for *using* LLMs effectively.\\n\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is LangChain?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"LangChain is a framework for developing applications powered by large language models (LLMs).  Think of it as a toolbox filled with different components that you can assemble to create sophisticated and useful applications.  These components handle things like:\\n\\n* **Connecting to LLMs:** LangChain makes it easy to interface with various LLMs, including OpenAI's models (like GPT-3, GPT-4), Hugging Face models, and others.  It abstracts away the complexities of making API calls and managing different API keys.  For example, you don't need to write custom code to handle authentication and rate limits for each LLM; LangChain provides a consistent interface.\\n\\n* **Managing Memory:**  LLMs are stateless, meaning they don't remember previous interactions within a single conversation. LangChain provides different memory mechanisms to allow your applications to maintain context across multiple turns, improving the coherence and usefulness of the LLM's responses.  These memory types include short-term memory (like remembering the last few messages using techniques like conversation buffers) and long-term memory (like storing information from previous sessions in a vector database).  This is particularly important for building conversational AI applications.\\n\\n* **Chains:**  This is a core concept in LangChain. Chains are sequences of calls to different components, often involving LLMs and other tools.  For example, a chain might involve:  (1) asking an LLM to generate a question, (2) using a search engine (like Google Search or SerpAPI) to find the answer, and (3) asking the LLM to summarize the answer.  This allows you to build complex workflows where the output of one component becomes the input of another.  LangChain provides pre-built chain types and allows you to create custom chains.\\n\\n* **Agents:**  Agents are more sophisticated than chains. They decide which tools to use to answer a user's query.  They might choose to use a search engine, a calculator, a database, or an LLM, depending on the context of the user's question.  The agent selects the appropriate tool based on a predefined set of rules or a learned policy. This makes your applications more adaptable and capable of handling a wider range of tasks.  For example, an agent might decide to use a calculator for a math problem and a search engine for a factual question.\\n\\n* **Indexes:**  LangChain helps you index and query your own data.  You can feed your documents (like PDFs, web pages, CSV files, etc.) into LangChain, and it will create an index (using techniques like embedding the text and storing it in a vector database) that allows the LLM to access and process that information.  This is crucial for building applications that leverage your own private data, enabling the LLM to answer questions based on your specific documents.  This avoids the need to manually parse and process your data.\\n\\nIn short, LangChain simplifies the process of building LLM-powered applications by providing pre-built components and tools for managing the complexities of interacting with LLMs and other external resources. It's designed to be modular and extensible, allowing developers to customize and combine components to create unique applications.  It's not an LLM itself, but rather a framework for *using* LLMs effectively.\\n\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is LangChain?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"LangChain is a framework for building applications powered by large language models (LLMs).  Think of it as a toolbox filled with various components you can assemble to create sophisticated applications.  These components handle key aspects of LLM application development:\\n\\n* **Connecting to LLMs:** LangChain simplifies connecting to different LLMs like OpenAI's GPT-3/4, Hugging Face models, etc.  It manages API calls, authentication, and rate limits, providing a consistent interface regardless of the specific LLM.  You don't need to write custom code for each LLM's API.\\n\\n* **Managing Memory:** Since LLMs are stateless, LangChain offers memory mechanisms to maintain context across multiple turns in a conversation.  This includes short-term memory (like remembering recent messages in a buffer) and long-term memory (persisting information across sessions, often using vector databases).  This is vital for building coherent conversational AI.\\n\\n* **Chains:**  These are sequences of calls to different components, often involving LLMs and other tools. A chain might involve: (1) an LLM generating a question, (2) a search engine finding the answer, and (3) the LLM summarizing the answer.  LangChain provides pre-built chain types and lets you create custom ones for complex workflows.\\n\\n* **Agents:**  Agents are more advanced than chains. They *decide* which tool (search engine, calculator, database, LLM, etc.) to use based on a user's query, adapting to different contexts.  This makes applications more versatile and capable of handling diverse tasks.\\n\\n* **Indexes:** LangChain helps you index your own data (PDFs, web pages, CSV files). It creates an index (often using embeddings and a vector database) allowing the LLM to access and process this information, enabling applications to leverage private data.\\n\\n\\nIn essence, LangChain simplifies building LLM applications by offering pre-built components and tools to manage the complexities of interacting with LLMs and external resources.  It's modular and extensible, letting developers customize and combine components.  It's not an LLM itself, but a framework for *using* LLMs effectively.\\n\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is LangChain?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"LangChain is a framework for building applications powered by large language models (LLMs).  Imagine it as a toolbox filled with pre-built components that you can assemble to create sophisticated applications.  These components handle several key aspects:\\n\\n* **Connecting to LLMs:** LangChain simplifies the process of connecting to various LLMs, such as OpenAI's GPT-3, GPT-4, and models from Hugging Face.  It abstracts away the complexities of API calls, authentication, and rate limits, providing a consistent interface regardless of the specific LLM you're using.  This means you don't have to write unique code for each LLM's API.\\n\\n* **Managing Memory:** Because LLMs are stateless (they don't inherently remember past interactions), LangChain provides memory mechanisms.  These allow your application to maintain context across multiple turns in a conversation.  This includes short-term memory (like keeping track of the last few messages) and long-term memory (persisting information across sessions, often using vector databases). This is crucial for building coherent conversational AI.\\n\\n* **Chains:**  Chains are sequences of calls to different components, often involving LLMs and other tools. A simple example:  (1) an LLM generates a question; (2) a search engine finds the answer; (3) the LLM summarizes the answer. LangChain offers pre-built chain types and allows you to create custom chains for complex workflows.\\n\\n* **Agents:** Agents are more advanced than chains.  They *decide* which tool to use (search engine, calculator, database, another LLM, etc.) based on the user's query.  This adaptability makes applications more versatile and capable of handling a wider range of tasks.\\n\\n* **Indexes:** LangChain helps you index your own data (like PDFs, web pages, or CSV files).  It creates an index (frequently using embeddings and a vector database) allowing the LLM to access and process this information. This is essential for applications that need to leverage private data.\\n\\nIn short, LangChain simplifies the development of LLM-powered applications by providing pre-built components and tools to manage the complexities of interacting with LLMs and external resources. It's designed to be modular and extensible, allowing developers to customize and combine components to build unique applications.  It's not an LLM itself; it's a framework for *using* LLMs effectively.\\n\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is LangChain?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"LangChain is a framework for building applications powered by large language models (LLMs).  Think of it as a toolbox filled with components you can assemble to create sophisticated applications.  These key components handle:\\n\\n* **Connecting to LLMs:** LangChain simplifies connecting to various LLMs like OpenAI's GPT-3/4, Hugging Face models, etc.  It manages API calls, authentication, and rate limits, offering a consistent interface regardless of the specific LLM. You don't write unique code for each LLM's API.\\n\\n* **Managing Memory:**  LLMs are stateless. LangChain provides memory mechanisms (short-term, like remembering recent messages; long-term, persisting information across sessions, often using vector databases) to maintain context across multiple conversational turns.  This is vital for coherent conversational AI.\\n\\n* **Chains:** These are sequences of calls to different components (often LLMs and other tools).  A chain might involve: (1) an LLM generating a question; (2) a search engine finding the answer; (3) the LLM summarizing the answer. LangChain offers pre-built and custom chain types for complex workflows.\\n\\n* **Agents:** Agents are advanced chains that *decide* which tool (search engine, calculator, database, another LLM) to use based on a user's query, adapting to different contexts for versatile application capabilities.\\n\\n* **Indexes:** LangChain helps index your own data (PDFs, web pages, CSV files) creating an index (often using embeddings and a vector database) that allows the LLM to access and process this information, enabling applications to leverage private data.\\n\\n\\nIn short, LangChain simplifies LLM application development by providing pre-built components and tools to manage the complexities of interacting with LLMs and external resources. It's modular, extensible, and allows developers to customize and combine components.  It's not an LLM itself, but a framework for *using* LLMs effectively.\\n\", additional_kwargs={}, response_metadata={})]\n",
            "Human: How does it work?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{'input': 'How does it work?', 'history': [HumanMessage(content='What is LangChain?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"LangChain is a framework for developing applications powered by large language models (LLMs).  Think of it as a toolbox filled with different components that you can assemble to create sophisticated and useful applications.  These components handle things like:\\n\\n* **Connecting to LLMs:** LangChain makes it easy to interface with various LLMs, including OpenAI's models (like GPT-3, GPT-4), Hugging Face models, and others.  It abstracts away the complexities of making API calls and managing different API keys.\\n\\n* **Managing Memory:**  LLMs are stateless, meaning they don't remember previous interactions within a single conversation. LangChain provides different memory mechanisms to allow your applications to maintain context across multiple turns, improving the coherence and usefulness of the LLM's responses.  These memory types include short-term memory (like remembering the last few messages) and long-term memory (like storing information from previous sessions).\\n\\n* **Chains:**  This is a core concept in LangChain. Chains are sequences of calls to different components, often involving LLMs and other tools.  For example, a chain might involve:  (1) asking an LLM to generate a question, (2) using a search engine to find the answer, and (3) asking the LLM to summarize the answer.  This allows you to build complex workflows.\\n\\n* **Agents:**  Agents are more sophisticated than chains. They decide which tools to use to answer a user's query.  They might choose to use a search engine, a calculator, or an LLM, depending on the context.  This makes your applications more adaptable and capable of handling a wider range of tasks.\\n\\n* **Indexes:**  LangChain helps you index and query your own data.  You can feed your documents (like PDFs, web pages, etc.) into LangChain, and it will create an index that allows the LLM to access and process that information.  This is crucial for building applications that leverage your own private data.\\n\\nIn short, LangChain simplifies the process of building LLM-powered applications by providing pre-built components and tools for managing the complexities of interacting with LLMs and other external resources. It's designed to be modular and extensible, allowing developers to customize and combine components to create unique applications.  It's not an LLM itself, but rather a framework for *using* LLMs effectively.\\n\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is LangChain?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"LangChain is a framework for developing applications powered by large language models (LLMs).  Think of it as a toolbox filled with different components that you can assemble to create sophisticated and useful applications.  These components handle things like:\\n\\n* **Connecting to LLMs:** LangChain makes it easy to interface with various LLMs, including OpenAI's models (like GPT-3, GPT-4), Hugging Face models, and others.  It abstracts away the complexities of making API calls and managing different API keys.  For example, you don't need to write custom code to handle authentication and rate limits for each LLM; LangChain provides a consistent interface.\\n\\n* **Managing Memory:**  LLMs are stateless, meaning they don't remember previous interactions within a single conversation. LangChain provides different memory mechanisms to allow your applications to maintain context across multiple turns, improving the coherence and usefulness of the LLM's responses.  These memory types include short-term memory (like remembering the last few messages using techniques like conversation buffers) and long-term memory (like storing information from previous sessions in a vector database).  This is particularly important for building conversational AI applications.\\n\\n* **Chains:**  This is a core concept in LangChain. Chains are sequences of calls to different components, often involving LLMs and other tools.  For example, a chain might involve:  (1) asking an LLM to generate a question, (2) using a search engine (like Google Search or SerpAPI) to find the answer, and (3) asking the LLM to summarize the answer.  This allows you to build complex workflows where the output of one component becomes the input of another.  LangChain provides pre-built chain types and allows you to create custom chains.\\n\\n* **Agents:**  Agents are more sophisticated than chains. They decide which tools to use to answer a user's query.  They might choose to use a search engine, a calculator, a database, or an LLM, depending on the context of the user's question.  The agent selects the appropriate tool based on a predefined set of rules or a learned policy. This makes your applications more adaptable and capable of handling a wider range of tasks.  For example, an agent might decide to use a calculator for a math problem and a search engine for a factual question.\\n\\n* **Indexes:**  LangChain helps you index and query your own data.  You can feed your documents (like PDFs, web pages, CSV files, etc.) into LangChain, and it will create an index (using techniques like embedding the text and storing it in a vector database) that allows the LLM to access and process that information.  This is crucial for building applications that leverage your own private data, enabling the LLM to answer questions based on your specific documents.  This avoids the need to manually parse and process your data.\\n\\nIn short, LangChain simplifies the process of building LLM-powered applications by providing pre-built components and tools for managing the complexities of interacting with LLMs and other external resources. It's designed to be modular and extensible, allowing developers to customize and combine components to create unique applications.  It's not an LLM itself, but rather a framework for *using* LLMs effectively.\\n\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is LangChain?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"LangChain is a framework for building applications powered by large language models (LLMs).  Think of it as a toolbox filled with various components you can assemble to create sophisticated applications.  These components handle key aspects of LLM application development:\\n\\n* **Connecting to LLMs:** LangChain simplifies connecting to different LLMs like OpenAI's GPT-3/4, Hugging Face models, etc.  It manages API calls, authentication, and rate limits, providing a consistent interface regardless of the specific LLM.  You don't need to write custom code for each LLM's API.\\n\\n* **Managing Memory:** Since LLMs are stateless, LangChain offers memory mechanisms to maintain context across multiple turns in a conversation.  This includes short-term memory (like remembering recent messages in a buffer) and long-term memory (persisting information across sessions, often using vector databases).  This is vital for building coherent conversational AI.\\n\\n* **Chains:**  These are sequences of calls to different components, often involving LLMs and other tools. A chain might involve: (1) an LLM generating a question, (2) a search engine finding the answer, and (3) the LLM summarizing the answer.  LangChain provides pre-built chain types and lets you create custom ones for complex workflows.\\n\\n* **Agents:**  Agents are more advanced than chains. They *decide* which tool (search engine, calculator, database, LLM, etc.) to use based on a user's query, adapting to different contexts.  This makes applications more versatile and capable of handling diverse tasks.\\n\\n* **Indexes:** LangChain helps you index your own data (PDFs, web pages, CSV files). It creates an index (often using embeddings and a vector database) allowing the LLM to access and process this information, enabling applications to leverage private data.\\n\\n\\nIn essence, LangChain simplifies building LLM applications by offering pre-built components and tools to manage the complexities of interacting with LLMs and external resources.  It's modular and extensible, letting developers customize and combine components.  It's not an LLM itself, but a framework for *using* LLMs effectively.\\n\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is LangChain?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"LangChain is a framework for building applications powered by large language models (LLMs).  Imagine it as a toolbox filled with pre-built components that you can assemble to create sophisticated applications.  These components handle several key aspects:\\n\\n* **Connecting to LLMs:** LangChain simplifies the process of connecting to various LLMs, such as OpenAI's GPT-3, GPT-4, and models from Hugging Face.  It abstracts away the complexities of API calls, authentication, and rate limits, providing a consistent interface regardless of the specific LLM you're using.  This means you don't have to write unique code for each LLM's API.\\n\\n* **Managing Memory:** Because LLMs are stateless (they don't inherently remember past interactions), LangChain provides memory mechanisms.  These allow your application to maintain context across multiple turns in a conversation.  This includes short-term memory (like keeping track of the last few messages) and long-term memory (persisting information across sessions, often using vector databases). This is crucial for building coherent conversational AI.\\n\\n* **Chains:**  Chains are sequences of calls to different components, often involving LLMs and other tools. A simple example:  (1) an LLM generates a question; (2) a search engine finds the answer; (3) the LLM summarizes the answer. LangChain offers pre-built chain types and allows you to create custom chains for complex workflows.\\n\\n* **Agents:** Agents are more advanced than chains.  They *decide* which tool to use (search engine, calculator, database, another LLM, etc.) based on the user's query.  This adaptability makes applications more versatile and capable of handling a wider range of tasks.\\n\\n* **Indexes:** LangChain helps you index your own data (like PDFs, web pages, or CSV files).  It creates an index (frequently using embeddings and a vector database) allowing the LLM to access and process this information. This is essential for applications that need to leverage private data.\\n\\nIn short, LangChain simplifies the development of LLM-powered applications by providing pre-built components and tools to manage the complexities of interacting with LLMs and external resources. It's designed to be modular and extensible, allowing developers to customize and combine components to build unique applications.  It's not an LLM itself; it's a framework for *using* LLMs effectively.\\n\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is LangChain?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"LangChain is a framework for building applications powered by large language models (LLMs).  Think of it as a toolbox filled with components you can assemble to create sophisticated applications.  These key components handle:\\n\\n* **Connecting to LLMs:** LangChain simplifies connecting to various LLMs like OpenAI's GPT-3/4, Hugging Face models, etc.  It manages API calls, authentication, and rate limits, offering a consistent interface regardless of the specific LLM. You don't write unique code for each LLM's API.\\n\\n* **Managing Memory:**  LLMs are stateless. LangChain provides memory mechanisms (short-term, like remembering recent messages; long-term, persisting information across sessions, often using vector databases) to maintain context across multiple conversational turns.  This is vital for coherent conversational AI.\\n\\n* **Chains:** These are sequences of calls to different components (often LLMs and other tools).  A chain might involve: (1) an LLM generating a question; (2) a search engine finding the answer; (3) the LLM summarizing the answer. LangChain offers pre-built and custom chain types for complex workflows.\\n\\n* **Agents:** Agents are advanced chains that *decide* which tool (search engine, calculator, database, another LLM) to use based on a user's query, adapting to different contexts for versatile application capabilities.\\n\\n* **Indexes:** LangChain helps index your own data (PDFs, web pages, CSV files) creating an index (often using embeddings and a vector database) that allows the LLM to access and process this information, enabling applications to leverage private data.\\n\\n\\nIn short, LangChain simplifies LLM application development by providing pre-built components and tools to manage the complexities of interacting with LLMs and external resources. It's modular, extensible, and allows developers to customize and combine components.  It's not an LLM itself, but a framework for *using* LLMs effectively.\\n\", additional_kwargs={}, response_metadata={}), HumanMessage(content='How does it work?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"LangChain works by providing a modular and extensible framework built around several core components, allowing developers to easily build applications that leverage the power of Large Language Models (LLMs).  Here's a breakdown of how it functions:\\n\\n1. **LLM Connection:**  The foundation is its ability to connect to various LLMs.  LangChain handles the complexities of interacting with different APIs (OpenAI, Hugging Face, etc.), abstracting away the details of authentication, API key management, and rate limiting.  This means you write code once to interact with an LLM, regardless of its provider.\\n\\n2. **Modular Components:**  LangChain is built from several reusable components:\\n\\n    * **LLMs:**  These are the interfaces to the various Large Language Models.  LangChain provides wrappers to make interacting with them consistent and simple.\\n\\n    * **Prompts:**  LangChain helps manage the creation and optimization of prompts for LLMs.  This includes features for prompt templating and chain-based prompt generation.\\n\\n    * **Chains:**  Chains are sequences of calls to different components.  They define a workflow, where the output of one component becomes the input of the next.  A simple chain might involve sending a prompt to an LLM, getting a response, and then sending that response to another component (like a search engine).  More complex chains can involve multiple LLMs, databases, and other tools.  LangChain offers pre-built chains for common tasks and allows you to create custom chains.\\n\\n    * **Agents:** Agents are more sophisticated than chains. They dynamically decide which component (or tool) to use based on the user's input and the context of the conversation.  This allows for more flexible and adaptable applications.  For example, an agent might decide to use a search engine for factual questions and a calculator for mathematical problems.  The agent's decision-making process can be rule-based or learned from data.\\n\\n    * **Indexes:**  LangChain allows you to index your own data (documents, databases, etc.) so that an LLM can access and process it.  This is done using techniques like embedding the text data and storing it in a vector database. This enables applications to leverage private knowledge bases.\\n\\n    * **Memory:**  Since LLMs are stateless, LangChain provides mechanisms for managing memory.  This allows applications to maintain context across multiple interactions, improving the coherence and flow of conversations.  Memory can be short-term (remembering recent messages) or long-term (persisting information across sessions).  Different memory types are available, such as ConversationBufferMemory and ConversationSummaryMemory.\\n\\n\\n3. **Workflow Orchestration:**  The real power comes from combining these components.  You can create complex workflows by chaining together LLMs, agents, indexes, and other tools.  LangChain handles the flow of data between these components.\\n\\n4. **Extensibility:**  The framework is designed to be easily extended.  You can add your own custom components or integrate with other libraries and services.\\n\\nIn essence, LangChain acts as an orchestrator, simplifying the process of building and managing complex LLM-powered applications by providing pre-built components and tools, managing the flow of data, and handling many of the common challenges associated with LLM integration.  It doesn't *do* anything on its own; it's a framework for *building* things that use LLMs effectively.\\n\", additional_kwargs={}, response_metadata={})], 'response': \"LangChain works by providing a modular and extensible framework built around several core components, allowing developers to easily build applications that leverage the power of Large Language Models (LLMs).  Here's a breakdown of how it functions:\\n\\n1. **LLM Connection:**  The foundation is its ability to connect to various LLMs.  LangChain handles the complexities of interacting with different APIs (OpenAI, Hugging Face, etc.), abstracting away the details of authentication, API key management, and rate limiting.  This means you write code once to interact with an LLM, regardless of its provider.\\n\\n2. **Modular Components:**  LangChain is built from several reusable components:\\n\\n    * **LLMs:**  These are the interfaces to the various Large Language Models.  LangChain provides wrappers to make interacting with them consistent and simple.\\n\\n    * **Prompts:**  LangChain helps manage the creation and optimization of prompts for LLMs.  This includes features for prompt templating and chain-based prompt generation.\\n\\n    * **Chains:**  Chains are sequences of calls to different components.  They define a workflow, where the output of one component becomes the input of the next.  A simple chain might involve sending a prompt to an LLM, getting a response, and then sending that response to another component (like a search engine).  More complex chains can involve multiple LLMs, databases, and other tools.  LangChain offers pre-built chains for common tasks and allows you to create custom chains.\\n\\n    * **Agents:** Agents are more sophisticated than chains. They dynamically decide which component (or tool) to use based on the user's input and the context of the conversation.  This allows for more flexible and adaptable applications.  For example, an agent might decide to use a search engine for factual questions and a calculator for mathematical problems.  The agent's decision-making process can be rule-based or learned from data.\\n\\n    * **Indexes:**  LangChain allows you to index your own data (documents, databases, etc.) so that an LLM can access and process it.  This is done using techniques like embedding the text data and storing it in a vector database. This enables applications to leverage private knowledge bases.\\n\\n    * **Memory:**  Since LLMs are stateless, LangChain provides mechanisms for managing memory.  This allows applications to maintain context across multiple interactions, improving the coherence and flow of conversations.  Memory can be short-term (remembering recent messages) or long-term (persisting information across sessions).  Different memory types are available, such as ConversationBufferMemory and ConversationSummaryMemory.\\n\\n\\n3. **Workflow Orchestration:**  The real power comes from combining these components.  You can create complex workflows by chaining together LLMs, agents, indexes, and other tools.  LangChain handles the flow of data between these components.\\n\\n4. **Extensibility:**  The framework is designed to be easily extended.  You can add your own custom components or integrate with other libraries and services.\\n\\nIn essence, LangChain acts as an orchestrator, simplifying the process of building and managing complex LLM-powered applications by providing pre-built components and tools, managing the flow of data, and handling many of the common challenges associated with LLM integration.  It doesn't *do* anything on its own; it's a framework for *building* things that use LLMs effectively.\\n\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(conversation.invoke(input=\"What is current market value of nvidia?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4PDzPZWlnDg",
        "outputId": "8e0f36fa-1f27-412f-d4db-cad6420e384c"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[HumanMessage(content='What is LangChain?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"LangChain is a framework for developing applications powered by large language models (LLMs).  Think of it as a toolbox filled with different components that you can assemble to create sophisticated and useful applications.  These components handle things like:\\n\\n* **Connecting to LLMs:** LangChain makes it easy to interface with various LLMs, including OpenAI's models (like GPT-3, GPT-4), Hugging Face models, and others.  It abstracts away the complexities of making API calls and managing different API keys.\\n\\n* **Managing Memory:**  LLMs are stateless, meaning they don't remember previous interactions within a single conversation. LangChain provides different memory mechanisms to allow your applications to maintain context across multiple turns, improving the coherence and usefulness of the LLM's responses.  These memory types include short-term memory (like remembering the last few messages) and long-term memory (like storing information from previous sessions).\\n\\n* **Chains:**  This is a core concept in LangChain. Chains are sequences of calls to different components, often involving LLMs and other tools.  For example, a chain might involve:  (1) asking an LLM to generate a question, (2) using a search engine to find the answer, and (3) asking the LLM to summarize the answer.  This allows you to build complex workflows.\\n\\n* **Agents:**  Agents are more sophisticated than chains. They decide which tools to use to answer a user's query.  They might choose to use a search engine, a calculator, or an LLM, depending on the context.  This makes your applications more adaptable and capable of handling a wider range of tasks.\\n\\n* **Indexes:**  LangChain helps you index and query your own data.  You can feed your documents (like PDFs, web pages, etc.) into LangChain, and it will create an index that allows the LLM to access and process that information.  This is crucial for building applications that leverage your own private data.\\n\\nIn short, LangChain simplifies the process of building LLM-powered applications by providing pre-built components and tools for managing the complexities of interacting with LLMs and other external resources. It's designed to be modular and extensible, allowing developers to customize and combine components to create unique applications.  It's not an LLM itself, but rather a framework for *using* LLMs effectively.\\n\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is LangChain?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"LangChain is a framework for developing applications powered by large language models (LLMs).  Think of it as a toolbox filled with different components that you can assemble to create sophisticated and useful applications.  These components handle things like:\\n\\n* **Connecting to LLMs:** LangChain makes it easy to interface with various LLMs, including OpenAI's models (like GPT-3, GPT-4), Hugging Face models, and others.  It abstracts away the complexities of making API calls and managing different API keys.  For example, you don't need to write custom code to handle authentication and rate limits for each LLM; LangChain provides a consistent interface.\\n\\n* **Managing Memory:**  LLMs are stateless, meaning they don't remember previous interactions within a single conversation. LangChain provides different memory mechanisms to allow your applications to maintain context across multiple turns, improving the coherence and usefulness of the LLM's responses.  These memory types include short-term memory (like remembering the last few messages using techniques like conversation buffers) and long-term memory (like storing information from previous sessions in a vector database).  This is particularly important for building conversational AI applications.\\n\\n* **Chains:**  This is a core concept in LangChain. Chains are sequences of calls to different components, often involving LLMs and other tools.  For example, a chain might involve:  (1) asking an LLM to generate a question, (2) using a search engine (like Google Search or SerpAPI) to find the answer, and (3) asking the LLM to summarize the answer.  This allows you to build complex workflows where the output of one component becomes the input of another.  LangChain provides pre-built chain types and allows you to create custom chains.\\n\\n* **Agents:**  Agents are more sophisticated than chains. They decide which tools to use to answer a user's query.  They might choose to use a search engine, a calculator, a database, or an LLM, depending on the context of the user's question.  The agent selects the appropriate tool based on a predefined set of rules or a learned policy. This makes your applications more adaptable and capable of handling a wider range of tasks.  For example, an agent might decide to use a calculator for a math problem and a search engine for a factual question.\\n\\n* **Indexes:**  LangChain helps you index and query your own data.  You can feed your documents (like PDFs, web pages, CSV files, etc.) into LangChain, and it will create an index (using techniques like embedding the text and storing it in a vector database) that allows the LLM to access and process that information.  This is crucial for building applications that leverage your own private data, enabling the LLM to answer questions based on your specific documents.  This avoids the need to manually parse and process your data.\\n\\nIn short, LangChain simplifies the process of building LLM-powered applications by providing pre-built components and tools for managing the complexities of interacting with LLMs and other external resources. It's designed to be modular and extensible, allowing developers to customize and combine components to create unique applications.  It's not an LLM itself, but rather a framework for *using* LLMs effectively.\\n\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is LangChain?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"LangChain is a framework for building applications powered by large language models (LLMs).  Think of it as a toolbox filled with various components you can assemble to create sophisticated applications.  These components handle key aspects of LLM application development:\\n\\n* **Connecting to LLMs:** LangChain simplifies connecting to different LLMs like OpenAI's GPT-3/4, Hugging Face models, etc.  It manages API calls, authentication, and rate limits, providing a consistent interface regardless of the specific LLM.  You don't need to write custom code for each LLM's API.\\n\\n* **Managing Memory:** Since LLMs are stateless, LangChain offers memory mechanisms to maintain context across multiple turns in a conversation.  This includes short-term memory (like remembering recent messages in a buffer) and long-term memory (persisting information across sessions, often using vector databases).  This is vital for building coherent conversational AI.\\n\\n* **Chains:**  These are sequences of calls to different components, often involving LLMs and other tools. A chain might involve: (1) an LLM generating a question, (2) a search engine finding the answer, and (3) the LLM summarizing the answer.  LangChain provides pre-built chain types and lets you create custom ones for complex workflows.\\n\\n* **Agents:**  Agents are more advanced than chains. They *decide* which tool (search engine, calculator, database, LLM, etc.) to use based on a user's query, adapting to different contexts.  This makes applications more versatile and capable of handling diverse tasks.\\n\\n* **Indexes:** LangChain helps you index your own data (PDFs, web pages, CSV files). It creates an index (often using embeddings and a vector database) allowing the LLM to access and process this information, enabling applications to leverage private data.\\n\\n\\nIn essence, LangChain simplifies building LLM applications by offering pre-built components and tools to manage the complexities of interacting with LLMs and external resources.  It's modular and extensible, letting developers customize and combine components.  It's not an LLM itself, but a framework for *using* LLMs effectively.\\n\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is LangChain?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"LangChain is a framework for building applications powered by large language models (LLMs).  Imagine it as a toolbox filled with pre-built components that you can assemble to create sophisticated applications.  These components handle several key aspects:\\n\\n* **Connecting to LLMs:** LangChain simplifies the process of connecting to various LLMs, such as OpenAI's GPT-3, GPT-4, and models from Hugging Face.  It abstracts away the complexities of API calls, authentication, and rate limits, providing a consistent interface regardless of the specific LLM you're using.  This means you don't have to write unique code for each LLM's API.\\n\\n* **Managing Memory:** Because LLMs are stateless (they don't inherently remember past interactions), LangChain provides memory mechanisms.  These allow your application to maintain context across multiple turns in a conversation.  This includes short-term memory (like keeping track of the last few messages) and long-term memory (persisting information across sessions, often using vector databases). This is crucial for building coherent conversational AI.\\n\\n* **Chains:**  Chains are sequences of calls to different components, often involving LLMs and other tools. A simple example:  (1) an LLM generates a question; (2) a search engine finds the answer; (3) the LLM summarizes the answer. LangChain offers pre-built chain types and allows you to create custom chains for complex workflows.\\n\\n* **Agents:** Agents are more advanced than chains.  They *decide* which tool to use (search engine, calculator, database, another LLM, etc.) based on the user's query.  This adaptability makes applications more versatile and capable of handling a wider range of tasks.\\n\\n* **Indexes:** LangChain helps you index your own data (like PDFs, web pages, or CSV files).  It creates an index (frequently using embeddings and a vector database) allowing the LLM to access and process this information. This is essential for applications that need to leverage private data.\\n\\nIn short, LangChain simplifies the development of LLM-powered applications by providing pre-built components and tools to manage the complexities of interacting with LLMs and external resources. It's designed to be modular and extensible, allowing developers to customize and combine components to build unique applications.  It's not an LLM itself; it's a framework for *using* LLMs effectively.\\n\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is LangChain?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"LangChain is a framework for building applications powered by large language models (LLMs).  Think of it as a toolbox filled with components you can assemble to create sophisticated applications.  These key components handle:\\n\\n* **Connecting to LLMs:** LangChain simplifies connecting to various LLMs like OpenAI's GPT-3/4, Hugging Face models, etc.  It manages API calls, authentication, and rate limits, offering a consistent interface regardless of the specific LLM. You don't write unique code for each LLM's API.\\n\\n* **Managing Memory:**  LLMs are stateless. LangChain provides memory mechanisms (short-term, like remembering recent messages; long-term, persisting information across sessions, often using vector databases) to maintain context across multiple conversational turns.  This is vital for coherent conversational AI.\\n\\n* **Chains:** These are sequences of calls to different components (often LLMs and other tools).  A chain might involve: (1) an LLM generating a question; (2) a search engine finding the answer; (3) the LLM summarizing the answer. LangChain offers pre-built and custom chain types for complex workflows.\\n\\n* **Agents:** Agents are advanced chains that *decide* which tool (search engine, calculator, database, another LLM) to use based on a user's query, adapting to different contexts for versatile application capabilities.\\n\\n* **Indexes:** LangChain helps index your own data (PDFs, web pages, CSV files) creating an index (often using embeddings and a vector database) that allows the LLM to access and process this information, enabling applications to leverage private data.\\n\\n\\nIn short, LangChain simplifies LLM application development by providing pre-built components and tools to manage the complexities of interacting with LLMs and external resources. It's modular, extensible, and allows developers to customize and combine components.  It's not an LLM itself, but a framework for *using* LLMs effectively.\\n\", additional_kwargs={}, response_metadata={}), HumanMessage(content='How does it work?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"LangChain works by providing a modular and extensible framework built around several core components, allowing developers to easily build applications that leverage the power of Large Language Models (LLMs).  Here's a breakdown of how it functions:\\n\\n1. **LLM Connection:**  The foundation is its ability to connect to various LLMs.  LangChain handles the complexities of interacting with different APIs (OpenAI, Hugging Face, etc.), abstracting away the details of authentication, API key management, and rate limiting.  This means you write code once to interact with an LLM, regardless of its provider.\\n\\n2. **Modular Components:**  LangChain is built from several reusable components:\\n\\n    * **LLMs:**  These are the interfaces to the various Large Language Models.  LangChain provides wrappers to make interacting with them consistent and simple.\\n\\n    * **Prompts:**  LangChain helps manage the creation and optimization of prompts for LLMs.  This includes features for prompt templating and chain-based prompt generation.\\n\\n    * **Chains:**  Chains are sequences of calls to different components.  They define a workflow, where the output of one component becomes the input of the next.  A simple chain might involve sending a prompt to an LLM, getting a response, and then sending that response to another component (like a search engine).  More complex chains can involve multiple LLMs, databases, and other tools.  LangChain offers pre-built chains for common tasks and allows you to create custom chains.\\n\\n    * **Agents:** Agents are more sophisticated than chains. They dynamically decide which component (or tool) to use based on the user's input and the context of the conversation.  This allows for more flexible and adaptable applications.  For example, an agent might decide to use a search engine for factual questions and a calculator for mathematical problems.  The agent's decision-making process can be rule-based or learned from data.\\n\\n    * **Indexes:**  LangChain allows you to index your own data (documents, databases, etc.) so that an LLM can access and process it.  This is done using techniques like embedding the text data and storing it in a vector database. This enables applications to leverage private knowledge bases.\\n\\n    * **Memory:**  Since LLMs are stateless, LangChain provides mechanisms for managing memory.  This allows applications to maintain context across multiple interactions, improving the coherence and flow of conversations.  Memory can be short-term (remembering recent messages) or long-term (persisting information across sessions).  Different memory types are available, such as ConversationBufferMemory and ConversationSummaryMemory.\\n\\n\\n3. **Workflow Orchestration:**  The real power comes from combining these components.  You can create complex workflows by chaining together LLMs, agents, indexes, and other tools.  LangChain handles the flow of data between these components.\\n\\n4. **Extensibility:**  The framework is designed to be easily extended.  You can add your own custom components or integrate with other libraries and services.\\n\\nIn essence, LangChain acts as an orchestrator, simplifying the process of building and managing complex LLM-powered applications by providing pre-built components and tools, managing the flow of data, and handling many of the common challenges associated with LLM integration.  It doesn't *do* anything on its own; it's a framework for *building* things that use LLMs effectively.\\n\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is current market value of nvidia?', additional_kwargs={}, response_metadata={}), AIMessage(content='I do not have access to real-time information, including live stock market data.  Therefore, I cannot provide you with the current market value of Nvidia.  To find this information, I recommend checking a reputable financial website such as Google Finance, Yahoo Finance, or the Nasdaq website.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Human: What is current market value of nvidia?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{'input': 'What is current market value of nvidia?', 'history': [HumanMessage(content='What is LangChain?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"LangChain is a framework for developing applications powered by large language models (LLMs).  Think of it as a toolbox filled with different components that you can assemble to create sophisticated and useful applications.  These components handle things like:\\n\\n* **Connecting to LLMs:** LangChain makes it easy to interface with various LLMs, including OpenAI's models (like GPT-3, GPT-4), Hugging Face models, and others.  It abstracts away the complexities of making API calls and managing different API keys.\\n\\n* **Managing Memory:**  LLMs are stateless, meaning they don't remember previous interactions within a single conversation. LangChain provides different memory mechanisms to allow your applications to maintain context across multiple turns, improving the coherence and usefulness of the LLM's responses.  These memory types include short-term memory (like remembering the last few messages) and long-term memory (like storing information from previous sessions).\\n\\n* **Chains:**  This is a core concept in LangChain. Chains are sequences of calls to different components, often involving LLMs and other tools.  For example, a chain might involve:  (1) asking an LLM to generate a question, (2) using a search engine to find the answer, and (3) asking the LLM to summarize the answer.  This allows you to build complex workflows.\\n\\n* **Agents:**  Agents are more sophisticated than chains. They decide which tools to use to answer a user's query.  They might choose to use a search engine, a calculator, or an LLM, depending on the context.  This makes your applications more adaptable and capable of handling a wider range of tasks.\\n\\n* **Indexes:**  LangChain helps you index and query your own data.  You can feed your documents (like PDFs, web pages, etc.) into LangChain, and it will create an index that allows the LLM to access and process that information.  This is crucial for building applications that leverage your own private data.\\n\\nIn short, LangChain simplifies the process of building LLM-powered applications by providing pre-built components and tools for managing the complexities of interacting with LLMs and other external resources. It's designed to be modular and extensible, allowing developers to customize and combine components to create unique applications.  It's not an LLM itself, but rather a framework for *using* LLMs effectively.\\n\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is LangChain?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"LangChain is a framework for developing applications powered by large language models (LLMs).  Think of it as a toolbox filled with different components that you can assemble to create sophisticated and useful applications.  These components handle things like:\\n\\n* **Connecting to LLMs:** LangChain makes it easy to interface with various LLMs, including OpenAI's models (like GPT-3, GPT-4), Hugging Face models, and others.  It abstracts away the complexities of making API calls and managing different API keys.  For example, you don't need to write custom code to handle authentication and rate limits for each LLM; LangChain provides a consistent interface.\\n\\n* **Managing Memory:**  LLMs are stateless, meaning they don't remember previous interactions within a single conversation. LangChain provides different memory mechanisms to allow your applications to maintain context across multiple turns, improving the coherence and usefulness of the LLM's responses.  These memory types include short-term memory (like remembering the last few messages using techniques like conversation buffers) and long-term memory (like storing information from previous sessions in a vector database).  This is particularly important for building conversational AI applications.\\n\\n* **Chains:**  This is a core concept in LangChain. Chains are sequences of calls to different components, often involving LLMs and other tools.  For example, a chain might involve:  (1) asking an LLM to generate a question, (2) using a search engine (like Google Search or SerpAPI) to find the answer, and (3) asking the LLM to summarize the answer.  This allows you to build complex workflows where the output of one component becomes the input of another.  LangChain provides pre-built chain types and allows you to create custom chains.\\n\\n* **Agents:**  Agents are more sophisticated than chains. They decide which tools to use to answer a user's query.  They might choose to use a search engine, a calculator, a database, or an LLM, depending on the context of the user's question.  The agent selects the appropriate tool based on a predefined set of rules or a learned policy. This makes your applications more adaptable and capable of handling a wider range of tasks.  For example, an agent might decide to use a calculator for a math problem and a search engine for a factual question.\\n\\n* **Indexes:**  LangChain helps you index and query your own data.  You can feed your documents (like PDFs, web pages, CSV files, etc.) into LangChain, and it will create an index (using techniques like embedding the text and storing it in a vector database) that allows the LLM to access and process that information.  This is crucial for building applications that leverage your own private data, enabling the LLM to answer questions based on your specific documents.  This avoids the need to manually parse and process your data.\\n\\nIn short, LangChain simplifies the process of building LLM-powered applications by providing pre-built components and tools for managing the complexities of interacting with LLMs and other external resources. It's designed to be modular and extensible, allowing developers to customize and combine components to create unique applications.  It's not an LLM itself, but rather a framework for *using* LLMs effectively.\\n\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is LangChain?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"LangChain is a framework for building applications powered by large language models (LLMs).  Think of it as a toolbox filled with various components you can assemble to create sophisticated applications.  These components handle key aspects of LLM application development:\\n\\n* **Connecting to LLMs:** LangChain simplifies connecting to different LLMs like OpenAI's GPT-3/4, Hugging Face models, etc.  It manages API calls, authentication, and rate limits, providing a consistent interface regardless of the specific LLM.  You don't need to write custom code for each LLM's API.\\n\\n* **Managing Memory:** Since LLMs are stateless, LangChain offers memory mechanisms to maintain context across multiple turns in a conversation.  This includes short-term memory (like remembering recent messages in a buffer) and long-term memory (persisting information across sessions, often using vector databases).  This is vital for building coherent conversational AI.\\n\\n* **Chains:**  These are sequences of calls to different components, often involving LLMs and other tools. A chain might involve: (1) an LLM generating a question, (2) a search engine finding the answer, and (3) the LLM summarizing the answer.  LangChain provides pre-built chain types and lets you create custom ones for complex workflows.\\n\\n* **Agents:**  Agents are more advanced than chains. They *decide* which tool (search engine, calculator, database, LLM, etc.) to use based on a user's query, adapting to different contexts.  This makes applications more versatile and capable of handling diverse tasks.\\n\\n* **Indexes:** LangChain helps you index your own data (PDFs, web pages, CSV files). It creates an index (often using embeddings and a vector database) allowing the LLM to access and process this information, enabling applications to leverage private data.\\n\\n\\nIn essence, LangChain simplifies building LLM applications by offering pre-built components and tools to manage the complexities of interacting with LLMs and external resources.  It's modular and extensible, letting developers customize and combine components.  It's not an LLM itself, but a framework for *using* LLMs effectively.\\n\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is LangChain?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"LangChain is a framework for building applications powered by large language models (LLMs).  Imagine it as a toolbox filled with pre-built components that you can assemble to create sophisticated applications.  These components handle several key aspects:\\n\\n* **Connecting to LLMs:** LangChain simplifies the process of connecting to various LLMs, such as OpenAI's GPT-3, GPT-4, and models from Hugging Face.  It abstracts away the complexities of API calls, authentication, and rate limits, providing a consistent interface regardless of the specific LLM you're using.  This means you don't have to write unique code for each LLM's API.\\n\\n* **Managing Memory:** Because LLMs are stateless (they don't inherently remember past interactions), LangChain provides memory mechanisms.  These allow your application to maintain context across multiple turns in a conversation.  This includes short-term memory (like keeping track of the last few messages) and long-term memory (persisting information across sessions, often using vector databases). This is crucial for building coherent conversational AI.\\n\\n* **Chains:**  Chains are sequences of calls to different components, often involving LLMs and other tools. A simple example:  (1) an LLM generates a question; (2) a search engine finds the answer; (3) the LLM summarizes the answer. LangChain offers pre-built chain types and allows you to create custom chains for complex workflows.\\n\\n* **Agents:** Agents are more advanced than chains.  They *decide* which tool to use (search engine, calculator, database, another LLM, etc.) based on the user's query.  This adaptability makes applications more versatile and capable of handling a wider range of tasks.\\n\\n* **Indexes:** LangChain helps you index your own data (like PDFs, web pages, or CSV files).  It creates an index (frequently using embeddings and a vector database) allowing the LLM to access and process this information. This is essential for applications that need to leverage private data.\\n\\nIn short, LangChain simplifies the development of LLM-powered applications by providing pre-built components and tools to manage the complexities of interacting with LLMs and external resources. It's designed to be modular and extensible, allowing developers to customize and combine components to build unique applications.  It's not an LLM itself; it's a framework for *using* LLMs effectively.\\n\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is LangChain?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"LangChain is a framework for building applications powered by large language models (LLMs).  Think of it as a toolbox filled with components you can assemble to create sophisticated applications.  These key components handle:\\n\\n* **Connecting to LLMs:** LangChain simplifies connecting to various LLMs like OpenAI's GPT-3/4, Hugging Face models, etc.  It manages API calls, authentication, and rate limits, offering a consistent interface regardless of the specific LLM. You don't write unique code for each LLM's API.\\n\\n* **Managing Memory:**  LLMs are stateless. LangChain provides memory mechanisms (short-term, like remembering recent messages; long-term, persisting information across sessions, often using vector databases) to maintain context across multiple conversational turns.  This is vital for coherent conversational AI.\\n\\n* **Chains:** These are sequences of calls to different components (often LLMs and other tools).  A chain might involve: (1) an LLM generating a question; (2) a search engine finding the answer; (3) the LLM summarizing the answer. LangChain offers pre-built and custom chain types for complex workflows.\\n\\n* **Agents:** Agents are advanced chains that *decide* which tool (search engine, calculator, database, another LLM) to use based on a user's query, adapting to different contexts for versatile application capabilities.\\n\\n* **Indexes:** LangChain helps index your own data (PDFs, web pages, CSV files) creating an index (often using embeddings and a vector database) that allows the LLM to access and process this information, enabling applications to leverage private data.\\n\\n\\nIn short, LangChain simplifies LLM application development by providing pre-built components and tools to manage the complexities of interacting with LLMs and external resources. It's modular, extensible, and allows developers to customize and combine components.  It's not an LLM itself, but a framework for *using* LLMs effectively.\\n\", additional_kwargs={}, response_metadata={}), HumanMessage(content='How does it work?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"LangChain works by providing a modular and extensible framework built around several core components, allowing developers to easily build applications that leverage the power of Large Language Models (LLMs).  Here's a breakdown of how it functions:\\n\\n1. **LLM Connection:**  The foundation is its ability to connect to various LLMs.  LangChain handles the complexities of interacting with different APIs (OpenAI, Hugging Face, etc.), abstracting away the details of authentication, API key management, and rate limiting.  This means you write code once to interact with an LLM, regardless of its provider.\\n\\n2. **Modular Components:**  LangChain is built from several reusable components:\\n\\n    * **LLMs:**  These are the interfaces to the various Large Language Models.  LangChain provides wrappers to make interacting with them consistent and simple.\\n\\n    * **Prompts:**  LangChain helps manage the creation and optimization of prompts for LLMs.  This includes features for prompt templating and chain-based prompt generation.\\n\\n    * **Chains:**  Chains are sequences of calls to different components.  They define a workflow, where the output of one component becomes the input of the next.  A simple chain might involve sending a prompt to an LLM, getting a response, and then sending that response to another component (like a search engine).  More complex chains can involve multiple LLMs, databases, and other tools.  LangChain offers pre-built chains for common tasks and allows you to create custom chains.\\n\\n    * **Agents:** Agents are more sophisticated than chains. They dynamically decide which component (or tool) to use based on the user's input and the context of the conversation.  This allows for more flexible and adaptable applications.  For example, an agent might decide to use a search engine for factual questions and a calculator for mathematical problems.  The agent's decision-making process can be rule-based or learned from data.\\n\\n    * **Indexes:**  LangChain allows you to index your own data (documents, databases, etc.) so that an LLM can access and process it.  This is done using techniques like embedding the text data and storing it in a vector database. This enables applications to leverage private knowledge bases.\\n\\n    * **Memory:**  Since LLMs are stateless, LangChain provides mechanisms for managing memory.  This allows applications to maintain context across multiple interactions, improving the coherence and flow of conversations.  Memory can be short-term (remembering recent messages) or long-term (persisting information across sessions).  Different memory types are available, such as ConversationBufferMemory and ConversationSummaryMemory.\\n\\n\\n3. **Workflow Orchestration:**  The real power comes from combining these components.  You can create complex workflows by chaining together LLMs, agents, indexes, and other tools.  LangChain handles the flow of data between these components.\\n\\n4. **Extensibility:**  The framework is designed to be easily extended.  You can add your own custom components or integrate with other libraries and services.\\n\\nIn essence, LangChain acts as an orchestrator, simplifying the process of building and managing complex LLM-powered applications by providing pre-built components and tools, managing the flow of data, and handling many of the common challenges associated with LLM integration.  It doesn't *do* anything on its own; it's a framework for *building* things that use LLMs effectively.\\n\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is current market value of nvidia?', additional_kwargs={}, response_metadata={}), AIMessage(content='I do not have access to real-time information, including live stock market data.  Therefore, I cannot provide you with the current market value of Nvidia.  To find this information, I recommend checking a reputable financial website such as Google Finance, Yahoo Finance, or the Nasdaq website.\\n', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is current market value of nvidia?', additional_kwargs={}, response_metadata={}), AIMessage(content='I do not have access to real-time information, including live stock market data.  Therefore, I cannot provide you with the current market value of Nvidia.  To find this information, I recommend checking a reputable financial website such as Google Finance, Yahoo Finance, or the Nasdaq website.\\n', additional_kwargs={}, response_metadata={})], 'response': 'I do not have access to real-time information, including live stock market data.  Therefore, I cannot provide you with the current market value of Nvidia.  To find this information, I recommend checking a reputable financial website such as Google Finance, Yahoo Finance, or the Nasdaq website.\\n'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.clear()"
      ],
      "metadata": {
        "id": "8K5XVZAqlv8s"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of switching topics\n",
        "print(conversation.run(input=\"What is LangChain?\"))  # Question about LangChain\n",
        "print(conversation.run(input=\"How does it work?\"))   # Follow-up question on LangChain\n",
        "print(conversation.run(input=\"What is the net worth of Elon Musk?\"))  # Switching topic\n",
        "print(conversation.run(input=\"Can you summarize what we discussed?\"))  # Asking for summary of LangChain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ghRMAhemGXi",
        "outputId": "2c7fd756-0a66-41a8-95df-f3712b16c931"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[]\n",
            "Human: What is LangChain?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "LangChain is a framework for developing applications powered by large language models (LLMs).  Think of it as a toolbox filled with different components that you can assemble to create sophisticated and useful applications.  Instead of just interacting with an LLM directly, LangChain provides a structured way to manage the interaction, allowing for more complex and robust functionality.\n",
            "\n",
            "Here's a breakdown of what makes LangChain special:\n",
            "\n",
            "* **Modular Design:**  It's built with modularity in mind.  This means you can easily swap out different components, like different LLMs or different ways of storing and retrieving information, without having to rewrite large portions of your code.  This promotes flexibility and allows you to experiment with different approaches.\n",
            "\n",
            "* **Chains:**  These are sequences of calls to LLMs or other utilities.  A simple chain might involve sending a prompt to an LLM and then returning the response.  More complex chains could involve multiple steps, such as retrieving information from a database, processing it, sending it to an LLM, and then formatting the output.  This allows for building multi-step workflows.\n",
            "\n",
            "* **Indexes:**  LangChain provides tools for indexing and querying various data sources.  This means you can connect your LLM to your own documents, databases, or other data stores, allowing the LLM to access and process information relevant to your specific needs.  This is crucial for building applications that are informed by your own data.\n",
            "\n",
            "* **Agents:**  These are components that allow your application to decide which tools to use and in what order.  Imagine needing to answer a question that requires looking up information on the web.  An agent could decide to use a search engine, retrieve relevant information, and then use an LLM to synthesize the results into a concise answer. This introduces decision-making capabilities into your applications.\n",
            "\n",
            "* **Memory:**  LangChain allows you to maintain a conversation history, giving your application a form of memory. This is essential for creating chatbots or other applications that need to remember previous interactions.\n",
            "\n",
            "In short, LangChain helps you build applications that go beyond simple prompt-response interactions with LLMs, enabling more sophisticated and useful interactions with data and external tools.  It's not an LLM itself, but a powerful framework for *using* LLMs effectively.\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[HumanMessage(content='What is LangChain?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"LangChain is a framework for developing applications powered by large language models (LLMs).  Think of it as a toolbox filled with different components that you can assemble to create sophisticated and useful applications.  Instead of just interacting with an LLM directly, LangChain provides a structured way to manage the interaction, allowing for more complex and robust functionality.\\n\\nHere's a breakdown of what makes LangChain special:\\n\\n* **Modular Design:**  It's built with modularity in mind.  This means you can easily swap out different components, like different LLMs or different ways of storing and retrieving information, without having to rewrite large portions of your code.  This promotes flexibility and allows you to experiment with different approaches.\\n\\n* **Chains:**  These are sequences of calls to LLMs or other utilities.  A simple chain might involve sending a prompt to an LLM and then returning the response.  More complex chains could involve multiple steps, such as retrieving information from a database, processing it, sending it to an LLM, and then formatting the output.  This allows for building multi-step workflows.\\n\\n* **Indexes:**  LangChain provides tools for indexing and querying various data sources.  This means you can connect your LLM to your own documents, databases, or other data stores, allowing the LLM to access and process information relevant to your specific needs.  This is crucial for building applications that are informed by your own data.\\n\\n* **Agents:**  These are components that allow your application to decide which tools to use and in what order.  Imagine needing to answer a question that requires looking up information on the web.  An agent could decide to use a search engine, retrieve relevant information, and then use an LLM to synthesize the results into a concise answer. This introduces decision-making capabilities into your applications.\\n\\n* **Memory:**  LangChain allows you to maintain a conversation history, giving your application a form of memory. This is essential for creating chatbots or other applications that need to remember previous interactions.\\n\\nIn short, LangChain helps you build applications that go beyond simple prompt-response interactions with LLMs, enabling more sophisticated and useful interactions with data and external tools.  It's not an LLM itself, but a powerful framework for *using* LLMs effectively.\\n\", additional_kwargs={}, response_metadata={})]\n",
            "Human: How does it work?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "LangChain works by providing a structured and modular framework for connecting and orchestrating various components to build applications around Large Language Models (LLMs).  It doesn't *do* anything on its own; instead, it acts as a glue that holds everything together.  Here's a more detailed breakdown of its operational mechanisms:\n",
            "\n",
            "1. **LLM Integration:** At its core, LangChain integrates with various LLMs.  This could be OpenAI's GPT models, Hugging Face models, or others.  You specify which LLM you want to use, and LangChain handles the communication with it.\n",
            "\n",
            "2. **Chains:**  These are the sequences of operations.  Imagine a chain as a pipeline.  Data flows through this pipeline, undergoing various transformations at each stage. A simple chain might just send a prompt to the LLM and return the response.  But more complex chains could involve multiple steps:\n",
            "\n",
            "    * **Prompt Generation:**  A component might create the prompt to send to the LLM, often incorporating information from other sources.\n",
            "    * **LLM Call:** The prompt is sent to the chosen LLM.\n",
            "    * **Response Processing:** The LLM's response is then processed  this could involve parsing, extracting key information, or formatting the output.\n",
            "    * **Data Retrieval (from Indexes):**  If the LLM needs additional information, a chain can access and retrieve relevant data from an index (explained below).\n",
            "    * **External Tool Calls (via Agents):**  The chain might call external tools like search engines or calculators, based on the decisions made by an agent (explained below).\n",
            "\n",
            "3. **Indexes:**  These are crucial for connecting LLMs to your own data.  LangChain provides tools to index various data sources like CSV files, PDFs, or databases.  This indexing process transforms your data into a format that the LLM can easily understand and query.  When a chain needs information, it can query the index.\n",
            "\n",
            "4. **Agents:**  These are decision-making components.  They determine which tools (LLMs, search engines, calculators, etc.) to use and in what order, based on the current context and the task at hand.  This allows for more complex workflows where the application dynamically chooses the best approach.  For example, an agent might decide to use a search engine to find relevant information before sending a query to an LLM.\n",
            "\n",
            "5. **Memory:**  LangChain provides mechanisms to maintain a conversation history or application state.  This allows the application to remember previous interactions, which is essential for creating chatbots or applications that need context across multiple turns.\n",
            "\n",
            "6. **Modular Design:**  All these components (LLMs, chains, indexes, agents, memory) are designed to be modular and interchangeable.  You can easily swap out different LLMs, use different indexing methods, or change the agents without significant code changes.\n",
            "\n",
            "In essence, LangChain acts as an orchestrator, managing the flow of data and interactions between different components to create sophisticated applications powered by LLMs.  It simplifies the process of building these applications by providing pre-built components and a structured way to connect them.\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[HumanMessage(content='What is LangChain?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"LangChain is a framework for developing applications powered by large language models (LLMs).  Think of it as a toolbox filled with different components that you can assemble to create sophisticated and useful applications.  Instead of just interacting with an LLM directly, LangChain provides a structured way to manage the interaction, allowing for more complex and robust functionality.\\n\\nHere's a breakdown of what makes LangChain special:\\n\\n* **Modular Design:**  It's built with modularity in mind.  This means you can easily swap out different components, like different LLMs or different ways of storing and retrieving information, without having to rewrite large portions of your code.  This promotes flexibility and allows you to experiment with different approaches.\\n\\n* **Chains:**  These are sequences of calls to LLMs or other utilities.  A simple chain might involve sending a prompt to an LLM and then returning the response.  More complex chains could involve multiple steps, such as retrieving information from a database, processing it, sending it to an LLM, and then formatting the output.  This allows for building multi-step workflows.\\n\\n* **Indexes:**  LangChain provides tools for indexing and querying various data sources.  This means you can connect your LLM to your own documents, databases, or other data stores, allowing the LLM to access and process information relevant to your specific needs.  This is crucial for building applications that are informed by your own data.\\n\\n* **Agents:**  These are components that allow your application to decide which tools to use and in what order.  Imagine needing to answer a question that requires looking up information on the web.  An agent could decide to use a search engine, retrieve relevant information, and then use an LLM to synthesize the results into a concise answer. This introduces decision-making capabilities into your applications.\\n\\n* **Memory:**  LangChain allows you to maintain a conversation history, giving your application a form of memory. This is essential for creating chatbots or other applications that need to remember previous interactions.\\n\\nIn short, LangChain helps you build applications that go beyond simple prompt-response interactions with LLMs, enabling more sophisticated and useful interactions with data and external tools.  It's not an LLM itself, but a powerful framework for *using* LLMs effectively.\\n\", additional_kwargs={}, response_metadata={}), HumanMessage(content='How does it work?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"LangChain works by providing a structured and modular framework for connecting and orchestrating various components to build applications around Large Language Models (LLMs).  It doesn't *do* anything on its own; instead, it acts as a glue that holds everything together.  Here's a more detailed breakdown of its operational mechanisms:\\n\\n1. **LLM Integration:** At its core, LangChain integrates with various LLMs.  This could be OpenAI's GPT models, Hugging Face models, or others.  You specify which LLM you want to use, and LangChain handles the communication with it.\\n\\n2. **Chains:**  These are the sequences of operations.  Imagine a chain as a pipeline.  Data flows through this pipeline, undergoing various transformations at each stage. A simple chain might just send a prompt to the LLM and return the response.  But more complex chains could involve multiple steps:\\n\\n    * **Prompt Generation:**  A component might create the prompt to send to the LLM, often incorporating information from other sources.\\n    * **LLM Call:** The prompt is sent to the chosen LLM.\\n    * **Response Processing:** The LLM's response is then processed  this could involve parsing, extracting key information, or formatting the output.\\n    * **Data Retrieval (from Indexes):**  If the LLM needs additional information, a chain can access and retrieve relevant data from an index (explained below).\\n    * **External Tool Calls (via Agents):**  The chain might call external tools like search engines or calculators, based on the decisions made by an agent (explained below).\\n\\n3. **Indexes:**  These are crucial for connecting LLMs to your own data.  LangChain provides tools to index various data sources like CSV files, PDFs, or databases.  This indexing process transforms your data into a format that the LLM can easily understand and query.  When a chain needs information, it can query the index.\\n\\n4. **Agents:**  These are decision-making components.  They determine which tools (LLMs, search engines, calculators, etc.) to use and in what order, based on the current context and the task at hand.  This allows for more complex workflows where the application dynamically chooses the best approach.  For example, an agent might decide to use a search engine to find relevant information before sending a query to an LLM.\\n\\n5. **Memory:**  LangChain provides mechanisms to maintain a conversation history or application state.  This allows the application to remember previous interactions, which is essential for creating chatbots or applications that need context across multiple turns.\\n\\n6. **Modular Design:**  All these components (LLMs, chains, indexes, agents, memory) are designed to be modular and interchangeable.  You can easily swap out different LLMs, use different indexing methods, or change the agents without significant code changes.\\n\\nIn essence, LangChain acts as an orchestrator, managing the flow of data and interactions between different components to create sophisticated applications powered by LLMs.  It simplifies the process of building these applications by providing pre-built components and a structured way to connect them.\\n\", additional_kwargs={}, response_metadata={})]\n",
            "Human: What is the net worth of Elon Musk?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "I do not have access to real-time information, including live financial data like Elon Musk's net worth.  Net worth fluctuates constantly based on stock prices and other market factors. To find his current net worth, I recommend checking reputable financial news websites such as Bloomberg, Forbes, or similar sources.\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[HumanMessage(content='What is LangChain?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"LangChain is a framework for developing applications powered by large language models (LLMs).  Think of it as a toolbox filled with different components that you can assemble to create sophisticated and useful applications.  Instead of just interacting with an LLM directly, LangChain provides a structured way to manage the interaction, allowing for more complex and robust functionality.\\n\\nHere's a breakdown of what makes LangChain special:\\n\\n* **Modular Design:**  It's built with modularity in mind.  This means you can easily swap out different components, like different LLMs or different ways of storing and retrieving information, without having to rewrite large portions of your code.  This promotes flexibility and allows you to experiment with different approaches.\\n\\n* **Chains:**  These are sequences of calls to LLMs or other utilities.  A simple chain might involve sending a prompt to an LLM and then returning the response.  More complex chains could involve multiple steps, such as retrieving information from a database, processing it, sending it to an LLM, and then formatting the output.  This allows for building multi-step workflows.\\n\\n* **Indexes:**  LangChain provides tools for indexing and querying various data sources.  This means you can connect your LLM to your own documents, databases, or other data stores, allowing the LLM to access and process information relevant to your specific needs.  This is crucial for building applications that are informed by your own data.\\n\\n* **Agents:**  These are components that allow your application to decide which tools to use and in what order.  Imagine needing to answer a question that requires looking up information on the web.  An agent could decide to use a search engine, retrieve relevant information, and then use an LLM to synthesize the results into a concise answer. This introduces decision-making capabilities into your applications.\\n\\n* **Memory:**  LangChain allows you to maintain a conversation history, giving your application a form of memory. This is essential for creating chatbots or other applications that need to remember previous interactions.\\n\\nIn short, LangChain helps you build applications that go beyond simple prompt-response interactions with LLMs, enabling more sophisticated and useful interactions with data and external tools.  It's not an LLM itself, but a powerful framework for *using* LLMs effectively.\\n\", additional_kwargs={}, response_metadata={}), HumanMessage(content='How does it work?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"LangChain works by providing a structured and modular framework for connecting and orchestrating various components to build applications around Large Language Models (LLMs).  It doesn't *do* anything on its own; instead, it acts as a glue that holds everything together.  Here's a more detailed breakdown of its operational mechanisms:\\n\\n1. **LLM Integration:** At its core, LangChain integrates with various LLMs.  This could be OpenAI's GPT models, Hugging Face models, or others.  You specify which LLM you want to use, and LangChain handles the communication with it.\\n\\n2. **Chains:**  These are the sequences of operations.  Imagine a chain as a pipeline.  Data flows through this pipeline, undergoing various transformations at each stage. A simple chain might just send a prompt to the LLM and return the response.  But more complex chains could involve multiple steps:\\n\\n    * **Prompt Generation:**  A component might create the prompt to send to the LLM, often incorporating information from other sources.\\n    * **LLM Call:** The prompt is sent to the chosen LLM.\\n    * **Response Processing:** The LLM's response is then processed  this could involve parsing, extracting key information, or formatting the output.\\n    * **Data Retrieval (from Indexes):**  If the LLM needs additional information, a chain can access and retrieve relevant data from an index (explained below).\\n    * **External Tool Calls (via Agents):**  The chain might call external tools like search engines or calculators, based on the decisions made by an agent (explained below).\\n\\n3. **Indexes:**  These are crucial for connecting LLMs to your own data.  LangChain provides tools to index various data sources like CSV files, PDFs, or databases.  This indexing process transforms your data into a format that the LLM can easily understand and query.  When a chain needs information, it can query the index.\\n\\n4. **Agents:**  These are decision-making components.  They determine which tools (LLMs, search engines, calculators, etc.) to use and in what order, based on the current context and the task at hand.  This allows for more complex workflows where the application dynamically chooses the best approach.  For example, an agent might decide to use a search engine to find relevant information before sending a query to an LLM.\\n\\n5. **Memory:**  LangChain provides mechanisms to maintain a conversation history or application state.  This allows the application to remember previous interactions, which is essential for creating chatbots or applications that need context across multiple turns.\\n\\n6. **Modular Design:**  All these components (LLMs, chains, indexes, agents, memory) are designed to be modular and interchangeable.  You can easily swap out different LLMs, use different indexing methods, or change the agents without significant code changes.\\n\\nIn essence, LangChain acts as an orchestrator, managing the flow of data and interactions between different components to create sophisticated applications powered by LLMs.  It simplifies the process of building these applications by providing pre-built components and a structured way to connect them.\\n\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the net worth of Elon Musk?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I do not have access to real-time information, including live financial data like Elon Musk's net worth.  Net worth fluctuates constantly based on stock prices and other market factors. To find his current net worth, I recommend checking reputable financial news websites such as Bloomberg, Forbes, or similar sources.\\n\", additional_kwargs={}, response_metadata={})]\n",
            "Human: Can you summarize what we discussed?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "We started by discussing LangChain, a framework for building applications using large language models (LLMs).  I explained its key features: modular design, chains (sequences of LLM calls and other operations), indexes (for connecting LLMs to external data), agents (for decision-making), and memory (for maintaining conversation history).  I then elaborated on how LangChain works, detailing its LLM integration, chain mechanisms, indexing methods, agent functionality, and memory management, emphasizing its role as an orchestrator of various components. Finally, you asked about Elon Musk's net worth, and I responded that I couldn't provide that real-time financial information, suggesting you check reputable financial news sources instead.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LfV7ith8mJjH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}